ggplot() +
geom_density(aes(x = sn)) +
geom_density(aes(x = n02), color = "red") +
geom_vline(xintercept = b_02, linetype = "dashed", size = 1.1) +
geom_vline(xintercept = -b_02, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(0, 2)") +
theme_base()
# plot the densities (boundaries have to be inserted but not so easy with one
# ggplot, maybe make two. Then it is just adding to vertical lines.)
ggplot() +
geom_density(aes(x = sn)) +
geom_density(aes(x = n02), color = "red") +
geom_vline(xintercept = b_02, linetype = "dashed", size = 1.1) +
geom_vline(xintercept = -b_02, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(0, 2)") +
theme_base()
# plot the densities (boundaries have to be inserted but not so easy with one
# ggplot, maybe make two. Then it is just adding to vertical lines.)
ggplot() +
geom_density(aes(x = sn)) +
geom_density(aes(x = n02), color = "red") +
geom_vline(xintercept = b_02, linetype = "dashed", size = 1.1) +
geom_vline(xintercept = -b_02, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(0, 2)") +
theme_base()
ggplot() +
geom_density(aes(x = sn)) +
geom_density(aes(x = n11), color = "red") +
geom_vline(xintercept = b_11, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(1, 1)") +
theme_base()
?pnorm
pnorm(0.5)
qnorm(0.5)
# the bayes rate for N(1, 1) is
0.5 * pnorm(0.5) + 0.5 * (1 - pnorm(0.5, 1, 1))
pnorm(0.5)
pnorm(0.5, 1, 1)
# the bayes rate for N(1, 1) is
0.5 * pnorm(0.5) + 0.5 * (1 - pnorm(0.5, 1, 1))
ggplot() +
geom_density(aes(x = sn)) +
geom_density(aes(x = n11), color = "red") +
geom_vline(xintercept = b_11, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(1, 1)") +
theme_base()
?integrate
integrate(dnorm, lower = -Inf, upper = 0.5, mean = 0, sd = 1))
integrate(dnorm, lower = -Inf, upper = 0.5, mean = 0, sd = 1)
pnorm(0.5) + (1 - pnorm(0.5, 1, 1))
# plot the densities (boundaries have to be inserted but not so easy with one
# ggplot, maybe make two. Then it is just adding to vertical lines.)
ggplot() +
geom_density(aes(x = sn)) +
geom_density(aes(x = n02), color = "red") +
geom_vline(xintercept = b_02, linetype = "dashed", size = 1.1) +
geom_vline(xintercept = -b_02, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(0, 2)") +
theme_base()
# the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 * (1 - pnorm(b_11, 1, 1))
# the bayes rate for N(0, 2) is
pnorm(-b_02, 0, 1) + integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2)) + (1 - pnorm(b_2, 0, 1))
integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))
str(integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2)))
# the bayes rate for N(0, 2) is
pnorm(-b_02, 0, 1) + integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value + (1 - pnorm(b_2, 0, 1))
# the bayes rate for N(0, 2) is
pnorm(-b_02, 0, 1) + integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value + (1 - pnorm(b_02, 0, 1))
# plot the densities (boundaries have to be inserted but not so easy with one
# ggplot, maybe make two. Then it is just adding to vertical lines.)
ggplot() +
geom_density(aes(x = sn), fill = "red") +
geom_density(aes(x = n02), color = "red") +
geom_vline(xintercept = b_02, linetype = "dashed", size = 1.1) +
geom_vline(xintercept = -b_02, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(0, 2)") +
theme_base()
-sqrt(2*log(2))
b_02 <- 2 * sqrt(log(sqrt(2)))
b_02
integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value
# the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 * (1 - pnorm(b_11, 1, 1))
# the bayes rate for N(0, 2) is
pnorm(-b_02, 0, 1) + integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value + (1 - pnorm(b_02, 0, 1))
pnorm(-b_02, 0, 1)
integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value
(1 - pnorm(b_02, 0, 1))
# the bayes rate for N(0, 2) is
0.5 * pnorm(-b_02, 0, 1) + 0.5* integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value + + 0.5 * (1 - pnorm(b_02, 0, 1))
# the bayes rate for N(0, 2) is
0.25 * pnorm(-b_02, 0, 1) + 0.5* integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value + + 0.25 * (1 - pnorm(b_02, 0, 1))
b_02 <- 2 * sqrt(log(sqrt(2)))
b_11 <- 0.5
# a) the bayes rate for N(0, 2) is
0.5 * pnorm(-b_02, 0, 1) + 0.5* integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value + + 0.5 * (1 - pnorm(b_02, 0, 1))
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 * (1 - pnorm(b_11, 1, 1))
# a) the bayes rate for N(0, 2) is
0.5 * pnorm(-b_02, 0, 1) + 0.5* integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value + + 0.5 * (1 - pnorm(b_02, 0, 1))
# plot the densities (boundaries have to be inserted but not so easy with one
# ggplot, maybe make two. Then it is just adding to vertical lines.)
ggplot() +
geom_density(aes(x = sn), fill = "red") +
geom_density(aes(x = n02), color = "red") +
geom_vline(xintercept = b_02, linetype = "dashed", size = 1.1) +
geom_vline(xintercept = -b_02, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(0, 2)") +
theme_base()
# plot the densities (boundaries have to be inserted but not so easy with one
# ggplot, maybe make two. Then it is just adding to vertical lines.)
ggplot() +
geom_density(aes(x = sn)) +
geom_density(aes(x = n02), color = "red") +
geom_vline(xintercept = b_02, linetype = "dashed", size = 1.1) +
geom_vline(xintercept = -b_02, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(0, 2)") +
theme_base()
# a) the bayes rate for N(0, 2) is
0.5 * pnorm(-b_02, 0, 1) + 0.5* integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value + + 0.5 * (1 - pnorm(b_02, 0, 1))
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 * (1 - pnorm(b_11, 1, 1))
# a) the bayes rate for N(0, 2) is
0.5 * pnorm(-b_02, 0, 1) + 0.5* integrate(dnorm, lower = -b_02, upper = b_02, mean = 0, sd = sqrt(2))$value + + 0.5 * (1 - pnorm(b_02, 0, 1))
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 * (1 - pnorm(b_11, 1, 1))
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 *  pnorm(b_11, 1, 1)
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 *  pnorm(b_11, 1, 1)
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 * (1 - pnorm(b_11, 1, 1))
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 * pnorm(b_11, 1, 1, lower.tail = T)
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 * pnorm(b_11, 1, 1, lower.tail = F)
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11) + 0.5 * pnorm(b_11, 1, 1, lower.tail = F)
?pnorm
# plot the densities (boundaries have to be inserted but not so easy with one
# ggplot, maybe make two. Then it is just adding to vertical lines.)
ggplot() +
geom_density(aes(x = sn)) +
geom_density(aes(x = n02), color = "red") +
geom_vline(xintercept = b_02, linetype = "dashed", size = 1.1) +
geom_vline(xintercept = -b_02, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(0, 2)") +
theme_base()
ggplot() +
geom_density(aes(x = sn)) +
geom_density(aes(x = n11), color = "red") +
geom_vline(xintercept = b_11, linetype = "dashed", size = 1.1) +
labs(title = "x ~ N(1, 1)") +
theme_base()
# b) the bayes rate for N(1, 1) is
0.5 * pnorm(b_11, 1, 1) + 0.5 * pnorm(b_11, 0, 1, lower.tail = F)
1/2 / 1/2
choose(1, 0)
choose(1, 1)
2 * sqrt(0.12)
(0.09 + 2 * sqrt(0.12)) / 0.15
# inspect SIGMA to check whether we did everything correctly
SIGMA
options(scipen = 999)
# Get the parameters from the task a d means that scaling to daily was performed
sigma <- c(0.2, 0.25)
sigma_d <- sigma / sqrt(250)
rho <- 0.4
cov <- rho * prod(sigma)
cov_d <- rho * prod(sigma_d)
SIGMA <- matrix(rep(NA, 4), ncol = 2)
SIGMA[diag(T, 2, 2)] <- sigma^2
SIGMA[!diag(T, 2, 2)] <- cov
# inspect SIGMA to check whether we did everything correctly
SIGMA
# inspect again for daily
SIGMA_d
SIGMA_d <- matrix(rep(NA, 4), ncol = 2)
SIGMA_d[diag(T, 2, 2)] <- sigma_d^2
SIGMA_d[!diag(T, 2, 2)] <- cov_d
# inspect again for daily
SIGMA_d
w <- matrix(c(0.6, 0.4), ncol = 1)
alpha <- 0.99
mu <- matrix(c(0, 0), ncol = 1)
V <- 10^6
V
# Get parameters for the distribution of the portfolio.
# Both log return series are normal -> linear combination
# is also normal. So
mu_portfolio <- t(w) %*% mu
var_portfolio <- t(w) %*% SIGMA %*% w
# We get VaR as the value of the portfolio multiplied with
# the 99% quantile of the loss distribution
VaR <- V * qnorm(0.99, mean = mu_portfolio, sd = sqrt(var_portfolio))
VaR
# As the portfolio returns are normal we can exploit that daily VaR is just
# annual VaR / sqrt(250)
VAR_daily <- VaR / sqrt(250)
VAR_daily
# mu stays as mu == [0, 0]'
var_d_portfolio <- t(w) %*% SIGMA_d %*% w
# We get VaR as the value of the portfolio multiplied with
# the 99% quantile of the loss distribution
VaR_d <- V * qnorm(0.99, mean = mu_portfolio, sd = sqrt(var_d_portfolio))
# For Expected Shortfall we have a closed form expression. We implement a
# function
ES_normal <- function(a = 0.99, mu = 0, sd = sqrt(var_d_portfolio)) {
mu + sd * dnorm((qnorm(p = a, mean = 0, sd = 1))) / (1 - a)
}
VaR_d
# For Expected Shortfall we have a closed form expression. We implement a
# function
ES_normal <- function(a = 0.99, mu = 0, sd = sqrt(var_d_portfolio)) {
mu + sd * dnorm((qnorm(p = a, mean = 0, sd = 1))) / (1 - a)
}
ES_daily <- ES_normal()
V * ES_daily
NaN
NaN/NaN
1/0/1/0
1/0
Inf/Inf
0/0
knitr::opts_chunk$set(echo = TRUE)
par(mfrow = c(1, 2))
{
par(mfrow = c(1, 2))
# d) plot for RIDGE
plot(RIDGE)
# d) plot with xvar = lambda
plot(RIDGE, xvar = "lambda")
}
library(MASS)
set.seed(123)
# a) draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 100), Sigma = diag(1, nrow = 100, ncol = 100))
names(X) <- paste0("X", 1:100)
# b) use 10 first columns of X and simulated noise to get y
y <- apply(X[, 1:10], MARGIN = 1, FUN = sum) + rnorm(100, mean = 0, sd = sqrt(0.01))
library(glmnet)
library(glmnet)
RIDGE <- glmnet(x = X, y = y, alpha = 0)
LASSO <- glmnet(x = X, y = y, alpha = 1)
par(mfrow = c(1, 2))
{
par(mfrow = c(1, 2))
# d) plot for RIDGE
plot(RIDGE)
# d) plot with xvar = lambda
plot(RIDGE, xvar = "lambda")
}
{
par(mfrow = c(2, 1))
# d) plot for RIDGE
plot(RIDGE)
# d) plot with xvar = lambda
plot(RIDGE, xvar = "lambda")
}
{
par(mfrow = c(1, 2))
# d) plot for LASSO
plot(LASSO)
# d) plot with xvar = lambda
plot(LASSO, xvar = "lambda")
}
# d) plot for LASSO
plot(LASSO)
# get deviance and lambda
RIDGE_dev_l <- data.frame(deviance = deviance(RIDGE), lambda = RIDGE$lambda)
LASSO_dev_l <- data.frame(deviance = deviance(LASSO), lambda = LASSO$lambda)
# get number of non-zero coefficients and lambda
# step 1 extract coefficients. Rows are Variables, columns correspond to lambda
RIDGE_coef <- coef(RIDGE)
LASSO_coef <- coef(LASSO)
# define an effective 0 as .Machine$double.eps and treat everything smaller as 0
# column sum of logical matrix provides number of non zero coefficients
RIDGE_coef_non_zero <- apply(abs(RIDGE_coef) > .Machine$double.eps, MARGIN = 2, sum)
LASSO_coef_non_zero <- apply(abs(LASSO_coef) > .Machine$double.eps, MARGIN = 2, sum)
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "l"))
with(LASSO_dev_l, plot(x = lambda, y = deviance, type = "l"))
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "l"))
with(LASSO_dev_l, plot(x = lambda, y = deviance, type = "l"))
# plot number of non-zeros in dependence of lambda
plot(x = RIDGE$lambda, y = RIDGE_coef_non_zero, type = "l")
plot(x = LASSO$lambda, y = LASSO_coef_non_zero, type = "l")
diag(100)
# d) plot for RIDGE
plot(RIDGE)
# a) draw from standard multivariate normal
X <- mvrnorm(n = 200, mu = rep(0, 100), Sigma = diag(1, nrow = 100, ncol = 100))
names(X) <- paste0("X", 1:100)
# b) use 10 first columns of X and simulated noise to get y
y <- apply(X[, 1:10], MARGIN = 1, FUN = sum) + rnorm(100, mean = 0, sd = sqrt(0.01))
library(glmnet)
RIDGE <- glmnet(x = X, y = y, alpha = 0)
LASSO <- glmnet(x = X, y = y, alpha = 1)
LASSO <- glmnet(x = X, y = y, alpha = 1)
```{r}
RIDGE <- glmnet(x = X, y = y, alpha = 0)
LASSO <- glmnet(x = X, y = y, alpha = 1)
# d) plot for RIDGE
plot(RIDGE)
# a) draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 200), Sigma = diag(1, nrow = 100, ncol = 100))
names(X) <- paste0("X", 1:100)
library(MASS)
set.seed(123)
# a) draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 200), Sigma = diag(1, nrow = 100, ncol = 100))
names(X) <- paste0("X", 1:100)
# a) draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 200), Sigma = diag(1, nrow = 200, ncol = 200))
names(X) <- paste0("X", 1:100)
names(X) <- paste0("X", 1:200)
# b) use 10 first columns of X and simulated noise to get y
y <- apply(X[, 1:10], MARGIN = 1, FUN = sum) + rnorm(100, mean = 0, sd = sqrt(0.01))
library(glmnet)
RIDGE <- glmnet(x = X, y = y, alpha = 0)
LASSO <- glmnet(x = X, y = y, alpha = 1)
# d) plot for RIDGE
plot(RIDGE)
library(MASS)
library(MASS)
set.seed(123)
# a) draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 100), Sigma = diag(1, nrow = 100, ncol = 100))
names(X) <- paste0("X", 1:100)
# b) use 10 first columns of X and simulated noise to get y
y <- apply(X[, 1:10], MARGIN = 1, FUN = sum) + rnorm(100, mean = 0, sd = sqrt(0.01))
library(glmnet)
RIDGE <- glmnet(x = X, y = y, alpha = 0)
LASSO <- glmnet(x = X, y = y, alpha = 1)
LASSO <- glmnet(x = X, y = y, alpha = 1)
```{r}
# d) plot with xvar = lambda
plot(RIDGE, xvar = "lambda")
# d) plot for RIDGE
plot(RIDGE)
# d) plot for RIDGE
plot(RIDGE)
# d) plot for LASSO
plot(LASSO)
?glmnet
RIDGE
# d) plot with xvar = lambda
plot(RIDGE, xvar = "lambda")
# d) plot with xvar = lambda
plot(RIDGE, xvar = "lambda")
# d) plot for LASSO
plot(LASSO)
# d) plot with xvar = lambda
plot(LASSO, xvar = "lambda")
log(1)
predict(LASSO, "nonzero")
predict(LASSO,type =  "nonzero")
plot(LASSO, xvar = "deviance")
plot(LASSO, xvar = "dev")
# get deviance and lambda
RIDGE_dev_l <- data.frame(deviance = deviance(RIDGE), lambda = RIDGE$lambda)
LASSO_dev_l <- data.frame(deviance = deviance(LASSO), lambda = LASSO$lambda)
with(LASSO_dev_l, plot(x = lambda, y = deviance, type = "l"))
plot(LASSO, xvar = "dev")
# get number of non-zero coefficients and lambda
# step 1 extract coefficients. Rows are Variables, columns correspond to lambda
RIDGE_coef <- coef(RIDGE)
LASSO_coef <- coef(LASSO)
# define an effective 0 as .Machine$double.eps and treat everything smaller as 0
# column sum of logical matrix provides number of non zero coefficients
RIDGE_coef_non_zero <- apply(abs(RIDGE_coef) > .Machine$double.eps, MARGIN = 2, sum)
LASSO_coef_non_zero <- apply(abs(LASSO_coef) > .Machine$double.eps, MARGIN = 2, sum)
RIDGE_coef_non_zero
LASSO_coef_non_zero
predict(LASSO, type = "nonzero")
predict(LASSO, type = "nonzero") %>% length()
lapply(predict(LASSO, type = "nonzero"), length)
# get number of non-zero coefficients and lambda
RIDGE_nz <- predict(RIDGE, type = "nonzero")
# get number of non-zero coefficients and lambda
RIDGE_nz <- lapply(predict(RIDGE, type = "nonzero"), length)
RIDGE_nz
# get number of non-zero coefficients and lambda
RIDGE_nz <- unlist(lapply(predict(RIDGE, type = "nonzero"), length))
RIDGE_nz
LASSO_nz <- unlist(lapply(predict(LASSO, type = "nonzero"), length))
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "l"))
with(LASSO_dev_l, plot(x = lambda, y = deviance, type = "l"))
# get deviance and lambda
RIDGE_dev_l <- data.frame(deviance = deviance(RIDGE), lambda = RIDGE$lambda)
LASSO_dev_l <- data.frame(deviance = deviance(LASSO), lambda = LASSO$lambda)
# get number of non-zero coefficients and lambda
RIDGE_nz <- unlist(lapply(predict(RIDGE, type = "nonzero"), length))
LASSO_nz <- unlist(lapply(predict(LASSO, type = "nonzero"), length))
# plot deviance in dependence of lambda for RIDGE
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "l"))
# plot number of non-zeros in dependence of lambda for RIDGE
plot(x = RIDGE$lambda, y = RIDGE_nz, type = "l")}
par(mfrow = c(1, 2))
{
par(mfrow = c(1, 2))
# plot deviance in dependence of lambda for RIDGE
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "l"))
# plot number of non-zeros in dependence of lambda for RIDGE
plot(x = RIDGE$lambda, y = RIDGE_nz, type = "l")
}
# plot number of non-zeros in dependence of lambda for RIDGE
plot(x = LASSOlambda, y = LASSO_nz, type = "l")
{
par(mfrow = c(1, 2))
# plot deviance in dependence of lambda for RIDGE
with(LASSO_dev_l, plot(x = lambda, y = deviance, type = "l"))
# plot number of non-zeros in dependence of lambda for RIDGE
plot(x = LASSOlambda, y = LASSO_nz, type = "l")
}
{
par(mfrow = c(1, 2))
# plot deviance in dependence of lambda for RIDGE
with(LASSO_dev_l, plot(x = lambda, y = deviance, type = "l"))
# plot number of non-zeros in dependence of lambda for RIDGE
plot(x = LASSO$lambda, y = LASSO_nz, type = "l")
}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library("ISLR2"))
suppressMessages(library("ggplot2"))
suppressMessages(library("gridExtra"))
suppressMessages(library("dplyr"))
set.seed(7362)
data("Default", package="ISLR2")
df=Default
#First, we need to convert the 'default' and 'student' variables into the
#binary format. "Yes"=1, "No"=0.
df$default = as.integer(ifelse(df$default == "Yes", 1, 0))
df$student = as.integer(ifelse(df$student == "Yes", 1, 0))
#We create an empty dataframe to store the results.
df_results = data.frame(matrix(ncol=3,nrow=100,
dimnames=list(NULL,
c("val_set_error",
"false_neg", "def_ratio"))))
# a)
glmfit_a = glm(default ~ income + balance, data = df, family =
binomial(link = "logit"))
print(summary(glmfit_a))
# b)
#Train-set split (70% train-30% test)
df$id = 1:nrow(df)
train = df %>% dplyr::sample_frac(0.70)
test = dplyr::anti_join(df, train, by = 'id')
# Training the logistic regression model
glmfit_b = glm(default ~ income + balance, data = train, family =
binomial(link = "logit"))
print(summary(glmfit_b))
# Getting the predictions for the test dataset
preds_test = predict(glmfit_b,test, type = "response")
#Since we get the probabilities with the regression model, we used a 0.5
#threshold.
preds = ifelse(preds_test>0.5, 1, 0)
#To compute the validation set error and false positive ratio we use the
#confusion matrix.
temp = table(test$default, preds)
print(temp)
#Misclassification rate, false negative ratio and default ratio
#calculations are stored in the dataframe.
val_set_error = (temp[2]+temp[3])/sum(temp)
false_neg = temp[2]/sum(temp)
def_ratio = sum(test$default)/sum(temp)
paste("Misclassification rate:",val_set_error)
paste("False negative rate:",false_neg)
paste("Default ratio:",def_ratio)
# c)
#We repeat the process 100 times using 100 different splits.
for(i in 1:100) {
#Train-set split (70% train-30% test)
df$id = 1:nrow(df)
train = df %>% dplyr::sample_frac(0.70)
test = dplyr::anti_join(df, train, by = 'id')
# Training the logistic regression model
glmfit = glm(default ~ income + balance, data = train, family =
binomial(link = "logit"))
# Getting the predictions for the test dataset
preds_test = predict(glmfit,test, type = "response")
#Since we get the probabilities with the regression model, we used a 0.5
#threshold.
preds = ifelse(preds_test>0.5, 1, 0)
#To compute the validation set error and false positive ratio we use the
#confusion matrix.
temp = table(test$default, preds)
#Misclassification rate, false negative ratio and default ratio
#calculations are stored in the dataframe.
df_results$val_set_error[i] = (temp[2]+temp[3])/sum(temp)
df_results$false_neg[i] = temp[2]/sum(temp)
df_results$def_ratio[i] = sum(test$default)/sum(temp)
}
# Creating the plots
val_set_error_plot = ggplot(df_results, aes(x=def_ratio, y=val_set_error)) +
geom_point() +
geom_smooth(method=lm , color="orange", se=FALSE) +
labs(title = "Validation Set Error vs. Default Ratio",
x = "Default Ratio of the Test Dataset",
y = "Validation Set Error") +
theme(plot.title = element_text(size=11))
false_neg_plot = ggplot(df_results, aes(x = def_ratio, y = false_neg)) +
geom_point() +
geom_smooth(method=lm , color="orange", se=FALSE) +
labs(title = "False Negative Rate vs. Default Ratio",
x = "Default Ratio of the Test Dataset",
y = "False Negative Rate") +
theme(plot.title = element_text(size=11))
grid.arrange(val_set_error_plot, false_neg_plot, nrow = 1)
