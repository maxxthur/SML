l_x0 <- predict(l, newdata = x0_df)
X <- as.data.frame(sapply(1:p, function(x) runif(n = N, min = -1, max = 1)))
epsilon <- rnorm(n = N, sd = sigma)
Y <- exp(-8 * apply(X^2, MARGIN = 1, FUN = sum)) + epsilon
get_xy <- function(p, sigma, N = 500) {
X <- as.data.frame(sapply(1:p, function(x) runif(n = N, min = -1, max = 1)))
epsilon <- rnorm(n = N, sd = sigma)
Y <- exp(-8 * apply(X^2, MARGIN = 1, FUN = sum)) + epsilon
return(data.frame(X = I(X), Y = Y))
}
x0_df <- matrix(rep(x0, ncol(data$X)), nrow = 1)
x0_df <- as.data.frame(matrix(rep(x0, ncol(data$X)), nrow = 1))
# estimate linear model
l <- lm(data = data, formula = Y ~ X)
names(x0_df) <- names(l$coefficients)[-1]
l_x0 <- predict(l, newdata = x0_df)
data
data$X
data = get_xy(p = 5, sigma = 1)
data
data
X <- sapply(1:p, function(x) runif(n = N, min = -1, max = 1))
get_xy <- function(p, sigma, N = 500) {
X <- sapply(1:p, function(x) runif(n = N, min = -1, max = 1))
epsilon <- rnorm(n = N, sd = sigma)
Y <- exp(-8 * apply(X^2, MARGIN = 1, FUN = sum)) + epsilon
return(data.frame(X = I(X), Y = Y))
}
data <- get_xy(p = 5, sigma = 1)
data
x0_df <- as.data.frame(matrix(rep(x0, ncol(data$X)), nrow = 1))
# estimate linear model
l <- lm(data = data, formula = Y ~ X)
names(x0_df) <- names(l$coefficients)[-1]
l_x0 <- predict(l, newdata = x0_df)
l_x0 <- predict.lm(l, newdata = x0_df)
X <- as.data.frame(data$X)
x0_df <- as.data.frame(matrix(rep(x0, ncol(X)), nrow = 1))
Y <- data$Y
X <- as.data.frame(data$X)
x0_df <- as.data.frame(matrix(rep(x0, ncol(X)), nrow = 1))
names(x0_df) <- names(l$coefficients)[-1]
names(x0_df) <- names(X)[-1]
# estimate linear model
l <- lm(data = data, formula = Y ~ X)
names(x0_df) <- names(l$coefficients)[-1]
# estimate linear model
l <- lm(data = data, formula = Y ~ X)
l_x0 <- predict.lm(l, newdata = x0_df)
l
x0_df
x0_df
ncol(X)
get_xy <- function(p, sigma, N = 500) {
X <- sapply(1:p, function(x) runif(n = N, min = -1, max = 1))
epsilon <- rnorm(n = N, sd = sigma)
Y <- exp(-8 * apply(X^2, MARGIN = 1, FUN = sum)) + epsilon
return(list(X = as.data.frame(X), Y = as.data.frame(Y)))
}
data <- get_xy(p = 5, sigma = 1)
data
Y <- data$Y
Y
Y
X <- data$X
X
x0_df <- as.data.frame(matrix(rep(x0, ncol(X)), nrow = 1))
x0_df
x0_df
# estimate linear model
l <- lm(data = data, formula = Y ~ X)
# estimate linear model
l <- lm(formula = Y ~ X)
Y <- data$Y
X <- data$X
x0_df <- as.data.frame(matrix(rep(x0, ncol(X)), nrow = 1))
# estimate linear model
l <- lm(formula = Y ~ X)
class(Y)
p = 5
sigma = 21
sigma = 1
N = 500
X <- sapply(1:p, function(x) runif(n = N, min = -1, max = 1))
epsilon <- rnorm(n = N, sd = sigma)
Y <- exp(-8 * apply(X^2, MARGIN = 1, FUN = sum)) + epsilon
df <- data.frame(Y = Y, X = X)
df
data <- get_xy(p = 5, sigma = 1)
data
x0_df <- as.data.frame(matrix(rep(x0, ncol(data-1)), nrow = 1))
x0_df <- as.data.frame(matrix(rep(x0, ncol(data)-1), nrow = 1))
ncol(data)
data
get_xy <- function(p, sigma, N = 500) {
X <- sapply(1:p, function(x) runif(n = N, min = -1, max = 1))
epsilon <- rnorm(n = N, sd = sigma)
Y <- exp(-8 * apply(X^2, MARGIN = 1, FUN = sum)) + epsilon
df <- data.frame(Y = Y, X = X)
return(df)
}
data
data
x0_df <- as.data.frame(matrix(rep(x0, ncol(data)-1), nrow = 1))
data
data <- get_xy(p = 5, sigma = 1)
data
x0_df <- as.data.frame(matrix(rep(x0, ncol(data)-1, nrow = 1))
# estimate linear model
l <- lm(formula = Y ~ X)
x0_df <- as.data.frame(matrix(rep(x0, ncol(data)-1, nrow = 1)))
x0_df
x0_df
x0_df <- as.data.frame(matrix(rep(x0, ncol(data)-1), nrow = 1))
x0_df
x0_df
data
names(data)
names(x0_df) <- names(data)[-1]
x0_df
x0_df
x0_df
x0_df <- as.data.frame(matrix(rep(x0, ncol(data)-1), nrow = 1))
names(x0_df) <- names(data)[-1]
f <- paste0(names(data)[1], "~", paste(names(data)[-1], collapse = "+"))
f
# estimate linear model
l <- lm(data = data, formula = f)
l
l_x0 <- predict.lm(l, newdata = x0_df)
l_x0
data[, -1]
data[, -1]
# but if we specify other x0 the function
# still works
# estimate knn
knn(train = data[, -1], test = x0_df, k = 1)
?knn
# but if we specify other x0 the function
# still works
# estimate knn
knn(train = data[, -1], cl = data[,1], test = x0_df, k = 1)
# but if we specify other x0 the function
# still works
# estimate knn
knn_x0 <- knn(train = data[, -1], cl = data[,1], test = x0_df, k = 1)
get_fx0 <- function(x0 = 0, sigma = 1, data) {
x0_df <- as.data.frame(matrix(rep(x0, ncol(data)-1), nrow = 1))
names(x0_df) <- names(data)[-1]
f <- paste0(names(data)[1], "~", paste(names(data)[-1], collapse = "+"))
# estimate linear model
l <- lm(data = data, formula = f)
l_x0 <- predict.lm(l, newdata = x0_df) # its just the intercept what was clear
# but if we specify other x0 the function
# still works
# estimate knn
knn_x0 <- knn(train = data[, -1], cl = data[,1], test = x0_df, k = 1)
# output the fx0 value
data.frame(linear = l_x0, knn = knn_x0)
}
knitr::opts_chunk$set(echo = TRUE)
# load data
data("diabetes", package = "lars")
# matrices can apparently be columns in df..
# good to know
y <- rnorm(100)
x1 = rnorm(100)
x2 = rnorm(100)
x <- cbind(x1, x2)
df <- data.frame(y = y, x = I(x))
# but this complicates things so we break up the
# structure and make a nice df
# function to get rid of Asis
unAsIs <- function(X) {
if("AsIs" %in% class(X)) {
class(X) <- class(X)[-match("AsIs", class(X))]
}
X
}
# extract y and x
y <- diabetes$y
x <- unAsIs(diabetes$x)
# make a new df with all the data
diabetes_df <- as.data.frame(cbind(y, x))
# set seed
set.seed(12)
# split the data into train and test
# step 1: sample 400 indices
ind <- sample(x = 1:nrow(diabetes_df), size = 400)
# subset the datasets as instructed
train <- diabetes_df[ind, ]
test <- diabetes_df[-ind, ]
library(kableExtra)
library(kableExtra)
# exploration of correlation
correlation_matrix <- round(cor(train), 2)
# eliminate redundancies and make a nice table for the pdf
correlation_matrix[!lower.tri(correlation_matrix)] <- ""
kable(correlation_matrix, booktabs = T)
# use training data to fit the full model
# get model formula from column names
f <- as.formula(paste0("y~", paste(colnames(train)[-1], collapse = "+")))
fit_full <- lm(data = train, formula = f)
# get variables significant at alpha = 0.05
summary_full <- summary(fit_full)
coefficients <- summary_full$coefficients
significant <- which(coefficients[, 4] < 0.05)[-1]
# in sample MSE
MSE_full_in <- mean(fit_full$residuals^2)
MSE_full_in
# out of sample MSE
pred_full <- predict(fit_full, newdata = test)
MSE_full_out <- mean((test$y - pred_full)^2)
MSE_full_out
# use the significant variables only
f2 <- as.formula(paste0("y~", paste(colnames(train)[significant], collapse = "+")))
# estimate smaller model
fit_sig <- lm(data = train, formula = f2)
# in sample MSE
MSE_sig_in <- mean(fit_sig$residuals^2)
# out of sample MSE
pred_sig <- predict(fit_sig, newdata = test)
MSE_sig_out <- mean((test$y - pred_sig)^2)
message(paste("In sample MSE is", MSE_sig_in, sep = ": "))
message(paste("Out of sample MSE is", MSE_sig_out, sep = ": "))
# write function to get formulas
bs_formulas <- function(x = train, dep = "y", intercept_only = T) {
# extract variables names
vars <- names(x)
exps <- vars[vars != dep]
# get all combinations
f <- lapply(1:length(exps), function(k) {
# get combinations for given k
combinations <- combn(exps, m = k, simplify = F)
# make it a regression formula
formulas <- lapply(combinations, function(c) {
paste(dep, paste(c, collapse = "+"), sep = "~")
})
# make it a vector again
unlist(formulas)
})
# dissolve list again
output <- unlist(f)
if(intercept_only == T) output <- c(as.formula(paste0(dep, "~ 1")), output)
return(output)
}
# get formulas
formulas <- bs_formulas() # look at defaults set above
# estimate all models
fits <- lapply(formulas, function(f) lm(data = train, formula = f))
# estimate all models
fits <- lapply(formulas, function(f) lm(data = train, formula = f))
# get log likelihoods
LL <- unlist(lapply(fits, function(f) logLik(f)[1]))
# get log likelihoods
LL <- unlist(lapply(fits, function(f) logLik(f)[1]))
# get number of regressors (K)
K <- unlist(lapply(fits, function(fit) length(fit$coefficients)))
# get AIC
AIC <- 2 * K - 2 * LL
# get the most appropriate model
best_index <- which.min(AIC)
# display it
formulas[[best_index]]
# get in sample MSE
MSE_BS_in <- mean(fits[[best_index]]$residuals^2)
# get out of sample MSE
MSE_BS_out <- mean((test$y - predict(fits[[best_index]], newdata = test))^2)
MSE_BS_in
MSE_BS_out
fit_full
MSE_full_in
MSE_full_out
# conduct backwards stepwise regression
backwards_step <- stepAIC(fit_full, direction = "backward")
library(MASS)
# conduct backwards stepwise regression
backwards_step <- stepAIC(fit_full, direction = "backward")
# get in sample MSE
MSE_backwards_in <- mean(backwards_step$residuals^2)
MSE_backwards_out <- mean((test$y - predict(backwards_step, newdata = test))^2)
MSE_backwards_in
backwards_step
backwards_step
backwards_step$coefficients
length(backwards_step$coefficients)
length(fit_full$coefficients)
backwards_step$residuals == fit_full$residuals
# conduct backwards stepwise regression
backwards_step <- stepAIC(fit_full, direction = "backward")
length(fit_full$coefficients)
# get in sample MSE
MSE_backwards_in <- mean(backwards_step$residuals^2)
MSE_backwards_in
MSE_backwards_in == MSE_full_in
MSE_backwards_in
MSE_full_in
MSE_backwards_out <- mean((test$y - predict(backwards_step, newdata = test))^2)
MSE_backwards_out
MSE_full_out
MSE_BS_in
MSE_BS_out
grid_split <- with(grid, split(x = grid, f = list(p, sigma)))
grid_split <- with(grid, split(x = grid, f = list("p", "sigma")))
# get all combinations of p and sigma
grid <- expand.grid(p = 1:10, sigma = 0:1)
grid_split <- with(grid, split(x = grid, f = list("p", "sigma")))
grid_split
grid_split
data <- lapply(grid_split, function(g) {
lapply(1:1000, get_xy, p = g$p, sigma = g$sigma)
})
get_xy <- function(p, sigma, N = 500) {
X <- sapply(1:p, function(x) runif(n = N, min = -1, max = 1))
epsilon <- rnorm(n = N, sd = sigma)
Y <- exp(-8 * apply(X^2, MARGIN = 1, FUN = sum)) + epsilon
df <- data.frame(Y = Y, X = X)
return(df)
}
data <- lapply(grid_split, function(g) {
lapply(1:1000, get_xy, p = g$p, sigma = g$sigma)
})
lapply(1:1000, get_xy, p = 1, sigma = 1)
lapply(1:1000, get_xy, p = 2, sigma = 1)
lapply(1:1000, get_xy, p = 2, sigma = 0)
lapply(1:1000, get_xy, p = 2, sigma = 1)
data <- lapply(grid_split, function(g) {
spec <- vector(mode = "list", length = 1000)
for(m in 1:1000) {
spec[[m]] <- get_xy(p = g$p, sigma = g$sigma)
}
})
data
# get all combinations of p and sigma
grid <- expand.grid(p = 1:10, sigma = 0:1)
grid_split <- with(grid, split(x = grid, f = list("p", "sigma")))
grid_split
grid_split
grid_split <- with(grid, split(x = grid, f = list(p, sigma)))
grid_split
grid_split
data <- lapply(grid_split, function(g) {
spec <- vector(mode = "list", length = 1000)
for(m in 1:1000) {
spec[[m]] <- get_xy(p = g$p, sigma = g$sigma)
}
})
data[[â€¢1]]
data[[1]]
data[1]
data
g <- grid_split[[1]]
g
spec <- vector(mode = "list", length = 1000)
spec
for(m in 1:1000) {
spec[[m]] <- get_xy(p = g$p, sigma = g$sigma)
}
spec
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(MASS)
set.seed(123)
?mvrnorm
# draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 100))
# draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 100), Sigma = 1)
X
knitr::opts_chunk$set(echo = TRUE)
f <- as.formula(paste0(names(data)[1], "~", paste(names(data)[-1], collapse = "+")))
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
# a) draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 100), Sigma = diag(1, nrow = 100, ncol = 100))
names(X) <- paste0("X", 1:100)
# b) use 10 first columns of X and simulated noise to get y
y <- apply(X[, 1:10], MARGIN = 1, FUN = sum) + rnorm(100, mean = 0, sd = sqrt(0.01))
set.seed(123)
# a) draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 100), Sigma = diag(1, nrow = 100, ncol = 100))
library(MASS)
set.seed(123)
# a) draw from standard multivariate normal
X <- mvrnorm(n = 100, mu = rep(0, 100), Sigma = diag(1, nrow = 100, ncol = 100))
names(X) <- paste0("X", 1:100)
# b) use 10 first columns of X and simulated noise to get y
y <- apply(X[, 1:10], MARGIN = 1, FUN = sum) + rnorm(100, mean = 0, sd = sqrt(0.01))
library(glmnet)
RIDGE <- glmnet(x = X, y = y, alpha = 0)
RIDGE
lambda_seq <- 1000:0
RIDGE <- glmnet(x = X, y = y, alpha = 0, lambda = lambda_seq)
RIDGE
LASSO <- glmnet(x = X, y = y, alpha = 1, lambda = lambda_seq)
LASSO
LASSO <- glmnet(x = X, y = y, alpha = 1)
LASSO <- glmnet(x = X, y = y, alpha = 1)
LASSO
# plot for RIDGE
plot(RIDGE)
# plot for LASSO
plot(LASSO)
# plot with xvar = lambda
plot(LASSO, xvar = "lambda")
# plot with xvar = lambda
plot(RIDGE, xvar = "lambda")
# determine number of non-zero coefficients
predict(RIDGE, type = "nonzero")
predict(RIDGE, type = "nonzero")
RIDGE$lambda
RIDGE <- glmnet(x = X, y = y, alpha = 0)
LASSO <- glmnet(x = X, y = y, alpha = 1)
RIDGE$lambda
RIDGE
deviance(RIDGE)
# get deviance and lambda
RIDGE_dev_l <- data.frame(deviance = deviance(RIDGE), lambda = RIDGE$lambda)
# get deviance and lambda
RIDGE_dev_l <- data.frame(deviance = deviance(RIDGE), lambda = RIDGE$lambda)
LASSO_dev_l <- data.frame(deviance = deviance(LASSO), lambda = LASSO$lambda)
predict(RIDGE, type = "nonzero")
View(predict(RIDGE, type = "nonzero"))
with(RIDGE_dev_l, plot(x = lambda, y = deviance))
with(LASSO_dev_l, plot(x = lambda, y = deviance))
coef(RIDGE)
length(RIDGE$lambda)
RIDGE <- glmnet(x = X, y = y, alpha = 0, lambda = 2)
length(RIDGE$lambda)
coef(RIDGE)
# get number of non-zero coefficients and lambda
# step 1 extract coefficients. Rows are Variables, columns correspond to lambda
RIDGE_coef <- coef(RIDGE)
LASSO_coef <- coef(LASSO)
# get number of non-zero coefficients and lambda
# step 1 extract coefficients. Rows are Variables, columns correspond to lambda
RIDGE_coef <- coef(RIDGE)
LASSO_coef <- coef(LASSO)
Machine$double.eps
Machine$double.eps
.Machine$double.eps
2.220446e-17
# define an effective 0 as .Machine$double.eps and treat everything smaller as 0
# column sum of logical matrix provides number of non zero coefficients
RIDGE_coef_non_zero <- apply(RIDGE_coef < .Machine$double.eps)
# define an effective 0 as .Machine$double.eps and treat everything smaller as 0
# column sum of logical matrix provides number of non zero coefficients
RIDGE_coef_non_zero <- apply(RIDGE_coef < .Machine$double.eps, MARGIN = 2, sum)
RIDGE_coef_non_zero
RIDGE_coef[[1]]
RIDGE_coef$s01
RIDGE_coef@x
RIDGE_coef
RIDGE <- glmnet(x = X, y = y, alpha = 0)
LASSO <- glmnet(x = X, y = y, alpha = 1)
LASSO <- glmnet(x = X, y = y, alpha = 1)
```{r}
# d) plot for RIDGE
plot(RIDGE)
# d) plot with xvar = lambda
plot(RIDGE, xvar = "lambda")
# get deviance and lambda
RIDGE_dev_l <- data.frame(deviance = deviance(RIDGE), lambda = RIDGE$lambda)
LASSO_dev_l <- data.frame(deviance = deviance(LASSO), lambda = LASSO$lambda)
# get number of non-zero coefficients and lambda
# step 1 extract coefficients. Rows are Variables, columns correspond to lambda
RIDGE_coef <- coef(RIDGE)
LASSO_coef <- coef(LASSO)
RIDGE_coef
# define an effective 0 as .Machine$double.eps and treat everything smaller as 0
# column sum of logical matrix provides number of non zero coefficients
RIDGE_coef_non_zero <- apply(RIDGE_coef < .Machine$double.eps, MARGIN = 2, sum)
RIDGE_coef_non_zero
# define an effective 0 as .Machine$double.eps and treat everything smaller as 0
# column sum of logical matrix provides number of non zero coefficients
RIDGE_coef_non_zero <- apply(RIDGE_coef > .Machine$double.eps, MARGIN = 2, sum)
LASSO_coef_non_zero <- apply(LASSO_coef > .Machine$double.eps, MARGIN = 2, sum)
RIDGE_coef_non_zero
LASSO_coef_non_zero
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance))
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance), type = "l")
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "l"))
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "ld"))
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "dl"))
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "pl"))
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "l"))
with(LASSO_dev_l, plot(x = lambda, y = deviance, type = "l"))
# plot number of non-zeros in dependence of lambda
plot(x = RIDGE$lambda, y = RIDGE_coef_non_zero)
# plot number of non-zeros in dependence of lambda
plot(x = RIDGE$lambda, y = RIDGE_coef_non_zero, type = "L")
# plot number of non-zeros in dependence of lambda
plot(x = RIDGE$lambda, y = RIDGE_coef_non_zero, type = "l")
plot(x = RIDGE$lambda, y = LASSO_coef_non_zero, type = "l")
# define an effective 0 as .Machine$double.eps and treat everything smaller as 0
# column sum of logical matrix provides number of non zero coefficients
RIDGE_coef_non_zero <- apply(RIDGE_coef > .Machine$double.eps, MARGIN = 2, sum)
LASSO_coef_non_zero <- apply(LASSO_coef > .Machine$double.eps, MARGIN = 2, sum)
# plot deviance in dependence of lambda
with(RIDGE_dev_l, plot(x = lambda, y = deviance, type = "l"))
with(LASSO_dev_l, plot(x = lambda, y = deviance, type = "l"))
# plot number of non-zeros in dependence of lambda
plot(x = RIDGE$lambda, y = RIDGE_coef_non_zero, type = "l")
plot(x = RIDGE$lambda, y = LASSO_coef_non_zero, type = "l")
plot(x = LASSO$lambda, y = LASSO_coef_non_zero, type = "l")
