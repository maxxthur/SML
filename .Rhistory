sample(x = y, size = n, replace = T)
})
# calculate the mean over the means
mean(means_b)
sd(means_b)
hist(means_b)
# draw 1000 samples from data generating process
bootstrap_true <- lapply(1:B, function(i) {
sample(x = y, size = n, replace = T)
})
bootstrap_true
rnorm(n = 10, mean = 0, sd = sqrt(3))
# draw 1000 samples from data generating process
bootstrap_true <- lapply(1:B, function(i) {
rnorm(n = 10, mean = 0, sd = sqrt(3))
})
# calculate the means for each bootstrap sample
means_b <- do.call("c", lapply(bootstrap_true, mean))
# calculate the mean over the means
mean(means_b)
sd(means_b)
hist(means_b)
mean(rnorm(10000, mean = 0, sd = sqrt(3)))
# draw 1000 samples from data generating process
bootstrap_true <- lapply(1:B, function(i) {
rnorm(n = 10, mean = 0, sd = sqrt(3))
})
# calculate the means for each bootstrap sample
means_b <- do.call("c", lapply(bootstrap_true, mean))
# calculate the mean over the means
mean(means_b)
sd(means_b)
# calculate the means of the union of the bootstrap samples
mean(do.call("c", bootstrap_true))
###################################################
### Example: spam
###################################################
data("spam", package = "ElemStatLearn")
library("rpart")
tree <- rpart(spam ~ ., data = spam,
method = "class", parms = list(split = "gini"),
control = list(cp = 0.0001))
tree$ordered
options(digits = 4, width= 60)
printcp(tree)
plotcp(tree)
ptree$frame
###################################################
### Example: spam
###################################################
data("spam", package = "ElemStatLearn")
library("rpart")
tree <- rpart(spam ~ ., data = spam,
method = "class", parms = list(split = "gini"),
control = list(cp = 0.0001))
options(digits = 4, width= 60)
printcp(tree)
plotcp(tree)
imin <- which.min(tree$cptable[, "xerror"])
select <- which(
tree$cptable[, "xerror"] <
sum(tree$cptable[imin, c("xerror", "xstd")]))[1]
ptree <- prune(tree, cp = tree$cptable[select, "CP"])
ptree <- prune(tree, cp = 0.01)
ptree
ptree $frame
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
# get 100 training data sets of length 100
training_data <- lapply(1:100, function(i) {
x <- rnorm(100)
noise <- rnorm(100, mean = 0, sd = sqrt(0.1))
y <- x + noise
data.frame(y = y, x = x)
})
# get 100 test data sets
test_data <- lapply(1:100, function(i) {
x <- rnorm(20)
noise <- rnorm(20, mean = 0, sd = sqrt(0.1))
y <- x + noise
data.frame(y = y, x = x)
})
# get OLS fits
linear_fits <- lapply(training_data, lm, formula = y ~ x)
library(caret)
library(rpart)
# fit trees
tree_fits <- lapply(training_data, function(d) {
train(y ~ x,
data = d,
method = "rpart")
})
test <- rpart(y ~ x,
data = training_data[[1]],
control = list(cp = .0001))
test$cptable
tree_fits[[1]]
tree_fits[[1]]$yLimits
tree_fits[[1]]$terms
tree_fits[[1]]$results
tree_fits[[1]]$bestTune
tree_fits[[1]]$finalModel
class(tree_fits[[1]]$finalModel)
tree_fits[[1]]$finalModel$frame
# fit trees
tree_fits <- lapply(training_data, function(d) {
# use caret for convenience, cost complexity
# pruning is conducted automatically and best
# model is by default chosen by RMSE
train(y ~ x,
data = d,
method = "rpart")
})
tree_fits
tree_fits[[1]]
tree_fits[[1]]$results
tree_fits[[1]]$finalModel
tree_fits[[1]]$finalModel$where
tree_fits[[1]]$finalModel$frame
# determine size as the number of nodes
tree_sizes <- lapply(tree_fits, function(t) {
# get frame and count non leaf entries
sum(t$finalModel$frame$var != "<leaf>")
})
tree_sizes
tree_fits[[1]]$finalModel
tree_fits[[2]]$finalModel
tree_fits
tree_fits[[1]]
predict(tree_fits[[1]], newdata = test_data[[1]])
test_data[[1]]
# get 100 test data sets
test_data <- lapply(1:1000, function(i) {
x <- rnorm(20)
noise <- rnorm(20, mean = 0, sd = sqrt(0.1))
y <- x + noise
data.frame(y = y, x = x)
})
?predict
lapply(test_data, predict, object = tree_fits[[1]])
test_data[[1]]
# get the prediction error by monte carlo simulation,
# i.e. by calculating the mean squared error for 1000 test
# data sets and averaging those again
prediction_error <- lapply(tree_fits, function(t) {
# get mean squared loss for each test data set
mean_squared_loss <- lapply(test_data, function(d) {
pred <- predict(t, newdata = d)
mse <- mean((d$y - pred)^2)
})
# prediction error
mean(unlist(mean_squared_loss))
})
# get the prediction error by monte carlo simulation,
# i.e. by calculating the mean squared error for 1000 test
# data sets and averaging those again
prediction_error <- lapply(tree_fits, function(t) {
# get mean squared loss for each test data set
mean_squared_loss <- lapply(test_data, function(d) {
pred <- predict(t, newdata = d)
mean((d$y - pred)^2)
})
# prediction error
mean(unlist(mean_squared_loss))
})
# get 100 test data sets
test_data <- lapply(1:100, function(i) {
x <- rnorm(20)
noise <- rnorm(20, mean = 0, sd = sqrt(0.1))
y <- x + noise
data.frame(y = y, x = x)
})
# get OLS fits
linear_fits <- lapply(training_data, lm, formula = y ~ x)
# get the prediction error by monte carlo simulation,
# i.e. by calculating the mean squared error for 1000 test
# data sets and averaging those again
prediction_error <- lapply(tree_fits, function(t) {
# get mean squared loss for each test data set
mean_squared_loss <- lapply(test_data, function(d) {
pred <- predict(t, newdata = d)
mean((d$y - pred)^2)
})
# prediction error
mean(unlist(mean_squared_loss))
})
prediction_error
# get OLS fits
linear_fits <- lapply(training_data, lm, formula = y ~ x)
lm_pred_err <- lapply(linear_fits, function(l) {
# get mean squared loss for each test data set
mean_squared_loss <- lapply(test_data, function(d) {
pred <- predict(l, newdata = d)
mean((d$y - pred)^2)
})
# prediction error
mean(unlist(mean_squared_loss))
})
lm_pred_err
tree_fits[[1]]
tree_fits[[1]]$results
tree_fits[[1]]$pred
tree_fits[[1]]$xlevels
tree_fits[[1]]$finalModel$where
tree_fits[[1]]$finalModel$splits
tree_fits[[1]]$finalModel$ordered
tree_fits[[1]]$finalModel$frame
# linear model
plot(training_data[[1]], linear_fits[[1]]$fitted.values)
# tree
plot(training_data[[1]], predict(tree_fits[[1]], newdata = training_data[[1]]))
# tree
plot(training_data[[1]]$y, predict(tree_fits[[1]], newdata = training_data[[1]]))
unlist(tree_size)
unlist(tree_sizes)
# linear model
plot(test_data[[1]]$y, predict(linear_fits[[1]], test_data[[1]]))
# tree
plot(test_data[[1]]$y, predict(tree_fits[[1]], newdata = test_data[[1]]))
pred_l <- predict(linear_fits[[1]], newdata = test_data[[1]])
pred_t <- predict(tree_fits[[1]], newdata = test_data[[1]])
pred_t
data <- test_data
data <- test_data[[1]]
data
data$t <- pred_t
---
title: "Assignment 3"
pred_l <- predict(linear_fits[[1]], newdata = test_data[[1]])
pred_t <- predict(tree_fits[[1]], newdata = test_data[[1]])
data <- test_data[[1]]
data$l <- pred_l
data$t <- pred_t
data
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y), color = "green")
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y), color = "black")
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y), color = "black") +
geom_line(aes(y = t), color = "red")
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y), color = "black") +
geom_line(aes(y = t), color = "red") +
geom_line(aes(y = l), color = "green")
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y)) +
geom_line(aes(y = t)) +
geom_line(aes(y = l)) +
scale_color_manual(values = c("true y" = "black", "tree" = "red", "linear" = "green"))
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y), color = "true y") +
geom_line(aes(y = t), color = "tree") +
geom_line(aes(y = l), color = "linear") +
scale_color_manual(values = c("true y" = "black", "tree" = "red", "linear" = "green"))
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y, color = "true y")) +
geom_line(aes(y = t, color = "tree")) +
geom_line(aes(y = l, color = "linear")) +
scale_color_manual(values = c("true y" = "black", "tree" = "red", "linear" = "green"))
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y, color = "true y")) +
geom_line(aes(y = t, color = "tree")) +
geom_line(aes(y = l, color = "linear")) +
scale_color_manual(values = c("true y" = "black", "tree" = "red", "linear" = "green")) %>%
theme_classic()
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y, color = "true y")) +
geom_line(aes(y = t, color = "tree")) +
geom_line(aes(y = l, color = "linear")) +
scale_color_manual(values = c("true y" = "black", "tree" = "red", "linear" = "green")) +
theme_classic()
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y, color = "true y")) +
geom_line(aes(y = t, color = "tree")) +
geom_line(aes(y = l, color = "linear")) +
scale_color_manual(values = c("true y" = "black", "tree" = "red", "linear" = "green")) +
theme_classic() +
theme(legend.title = element_blank())
table(unlist(tree_sizes))
# get 100 training data sets of length 100
training_data <- lapply(1:100, function(i) {
x <- rnorm(100)
noise <- rnorm(100, mean = 0, sd = sqrt(0.1))
y <- x + noise
data.frame(y = y, x = x)
})
# get 100 test data sets
test_data <- lapply(1:100, function(i) {
x <- rnorm(20)
noise <- rnorm(20, mean = 0, sd = sqrt(0.1))
y <- x + noise
data.frame(y = y, x = x)
})
# get 100 test data sets
test_data <- lapply(1:100, function(i) {
x <- rnorm(20)
noise <- rnorm(20, mean = 0, sd = sqrt(0.1))
y <- x + noise
data.frame(y = y, x = x)
})
# get OLS fits
linear_fits <- lapply(training_data, lm, formula = y ~ x)
lm_pred_err <- lapply(linear_fits, function(l) {
# get mean squared loss for each test data set
mean_squared_loss <- lapply(test_data, function(d) {
pred <- predict(l, newdata = d)
mean((d$y - pred)^2)
})
# prediction error
mean(unlist(mean_squared_loss))
})
library(caret)
library(rpart)
# fit trees
tree_fits <- lapply(training_data, function(d) {
# use caret for convenience, cost complexity
# pruning is conducted automatically and best
# model is by default chosen by RMSE
train(y ~ x,
data = d,
method = "rpart")
})
# determine size as the number of nodes
tree_sizes <- lapply(tree_fits, function(t) {
# get frame and count non leaf entries
sum(t$finalModel$frame$var != "<leaf>")
})
# get the prediction error by monte carlo simulation,
# i.e. by calculating the mean squared error for 1000 test
# data sets and averaging those again
tree_pred_err <- lapply(tree_fits, function(t) {
# get mean squared loss for each test data set
mean_squared_loss <- lapply(test_data, function(d) {
pred <- predict(t, newdata = d)
mean((d$y - pred)^2)
})
# prediction error
mean(unlist(mean_squared_loss))
})
prediction_errors <- data.frame(linear = unlist(lm_pred_err),
tree = unlist(tree_pred_err))
prediction_errors
prediction_error %>%
pivot_longer()
prediction_error %>%
pivot_longer(cols = everything())
prediction_errors %>%
pivot_longer(cols = everything())
prediction_errors %>%
pivot_longer(cols = everything()) %>%
ggplot(aes(x = value)) +
geom_histogram()
prediction_errors %>%
pivot_longer(cols = everything()) %>%
ggplot(aes(x = value)) +
geom_histogram(, bins = 200)
prediction_errors %>%
pivot_longer(cols = everything()) %>%
ggplot(aes(x = value, fill = name)) +
geom_histogram(, bins = 200)
prediction_errors %>%
pivot_longer(cols = everything()) %>%
ggplot(aes(x = value, fill = name)) +
geom_histogram(, bins = 200) +
theme(legend.title = element_blank())
prediction_errors %>%
pivot_longer(cols = everything()) %>%
ggplot(aes(x = value, fill = name)) +
geom_histogram(bins = 200) +
theme(legend.title = element_blank())
prediction_errors %>%
pivot_longer(cols = everything()) %>%
ggplot(aes(x = value, fill = name)) +
geom_histogram(bins = 200) +
theme(legend.title = element_blank()) +
theme_classic()
prediction_errors %>%
pivot_longer(cols = everything()) %>%
ggplot(aes(x = value, fill = name)) +
geom_histogram(bins = 200) +
theme(legend.title = element_blank()) +
theme_bw()
prediction_errors %>%
pivot_longer(cols = everything()) %>%
ggplot(aes(x = value, fill = name)) +
geom_histogram(bins = 200) +
theme(legend.title = element_blank()) +
theme_bw() +
labs(x = "Prediction Error")
## Task 9
?grViz
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
library(DiagrammeR)
?grViz
install.packages("DiagrammeR")
install.packages("DiagrammeR")
library(DiagrammeR)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
# Create a decision tree structure with circles of the same size and True/False labels
decision_tree <- "
"
# Create a graph from the decision tree structure
tree_graph <- grViz(decision_tree)
# Display the decision tree
tree_graph
tree_graph
install.packages("webshot")
library(webshot)
install.packages("webshot")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
library(webshot)
webshot::install_phantomjs()
set.seed(123)
# get 100 training data sets of length 100
training_data <- lapply(1:100, function(i) {
x <- rnorm(100)
noise <- rnorm(100, mean = 0, sd = sqrt(0.1))
y <- x + noise
data.frame(y = y, x = x)
})
# get 100 test data sets
test_data <- lapply(1:100, function(i) {
x <- rnorm(20)
noise <- rnorm(20, mean = 0, sd = sqrt(0.1))
y <- x + noise
data.frame(y = y, x = x)
})
# get OLS fits
linear_fits <- lapply(training_data, lm, formula = y ~ x)
lm_pred_err <- lapply(linear_fits, function(l) {
# get mean squared loss for each test data set
mean_squared_loss <- lapply(test_data, function(d) {
pred <- predict(l, newdata = d)
mean((d$y - pred)^2)
})
# prediction error
mean(unlist(mean_squared_loss))
})
library(caret)
# fit trees
tree_fits <- lapply(training_data, function(d) {
# use caret for convenience, cost complexity
# pruning is conducted automatically and best
# model is by default chosen by RMSE
train(y ~ x,
data = d,
method = "rpart")
})
# determine size as the number of nodes
tree_sizes <- lapply(tree_fits, function(t) {
# get frame and count non leaf entries
sum(t$finalModel$frame$var != "<leaf>")
})
# get the prediction error by monte carlo simulation,
# i.e. by calculating the mean squared error for 1000 test
# data sets and averaging those again
tree_pred_err <- lapply(tree_fits, function(t) {
# get mean squared loss for each test data set
mean_squared_loss <- lapply(test_data, function(d) {
pred <- predict(t, newdata = d)
mean((d$y - pred)^2)
})
# prediction error
mean(unlist(mean_squared_loss))
})
pred_l <- predict(linear_fits[[1]], newdata = test_data[[1]])
pred_t <- predict(tree_fits[[1]], newdata = test_data[[1]])
data <- test_data[[1]]
data$l <- pred_l
data$t <- pred_t
data %>%
ggplot(aes(x = x)) +
geom_point(aes(y = y, color = "true y")) +
geom_line(aes(y = t, color = "tree")) +
geom_line(aes(y = l, color = "linear")) +
scale_color_manual(values = c("true y" = "black", "tree" = "red", "linear" = "green")) +
theme_classic() +
theme(legend.title = element_blank())
table(unlist(tree_sizes))
install.packages("keras")
library(keras)
data("IMDb", package = "keras")
load_data("IMDb", package = "keras")
dataset_imdb(
path = "imdb.npz",
num_words = NULL,
skip_top = 0L,
maxlen = NULL,
seed = 113L,
start_char = 1L,
oov_char = 2L,
index_from = 3L
)
(82.79/81.28 -1) * 365/20
