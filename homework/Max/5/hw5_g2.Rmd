---
title: "Assignment 3"
author: "Group 2"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
library(mvtnorm)
library(nnet)
library(keras)
library(reticulate)
```

## Task 1

## Task 2

## Task 3

## Task 4

## Task 5

## Task 6

## Task 7

## Task 8

```{r}
# setup
a1 <- matrix(c(3, 3), ncol = 1) # already transposed
a2 <- matrix(c(3, -3), ncol = 1) # already transposed
sigmoid <- function(v) {
  1 / (1 + exp(-v))
}

# generate X_i
X1 <- rmvnorm(mean = c(0, 0), sigma = diag(2), n = 10100)
X2 <- rmvnorm(mean = c(0, 0), sigma = diag(2), n = 10100)

# as independent sampling, first 100 are training and rest is 
fX <- sigmoid(X1 %*% a1 ) + sigmoid(X2 %*% a2)

# signal-to-noise ratio is Var(fX)/Var(noise) = 4
var_noise <- var(fX)/4

# as you didnt specify mu for noise we assume 0
noise <- rnorm(n = 10100, sd = sqrt(var_noise))

# make training data 
df <- tibble(Y = fX + noise, X1 = I(X1), X2 = I(X2))
train <- df[1:100, ]

# fit the neural nets
results <- lapply(0:10, function(size) {
  
  weights <- lapply(1:10, function(x) abs(rnorm(100, mean = 0, sd = 0.01))) # generate random numbers near 0
  
  fits <- lapply(weights, function(w) nnet(formula = Y ~ ., data = train, size = size, decay = 0.0005, skip = T, weights = w))
  preds <- lapply(fits, predict, newdata = df[-(1:100), ])
  avg_train_err <- lapply(preds, function(p) mean((df[-(1:100),]$Y - p)^2))
  data.frame(size = size, err = unlist(avg_train_err))
})

# bind the results 
results_bind <- bind_rows(results) 
means <- results_bind %>% 
  group_by(size) %>% 
  summarise(err = median(err))

# visualize with box and line plot
results_bind %>% 
  ggplot() +
  geom_boxplot(aes(x = size, y = err, group = size), width = 0.25) +
  geom_line(data = means, aes(x = size, y = err)) +
  scale_x_continuous(breaks = 0:10)

```


## Task 9

## Task 10

```{r, results='hide'}
library(keras)

# load the data for different dictionary sizes and prepare it for analysis
dic_size <- c(500, 1000, 3000, 5000, 10000)

vectorize_sequences <- function(sequences, dimension) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1 
  results
}

dics <- lapply(dic_size, function(d) {
  imdb <- dataset_imdb(num_words = d)
  x_train <- vectorize_sequences(imdb$train$x, dimension = d)
  x_test <- vectorize_sequences(imdb$test$x, dimension = d)
  # Also change labels from integer to numeric
  y_train <- as.numeric(imdb$train$y)
  y_test <- as.numeric(imdb$test$y)
  list(xtrain = x_train, 
       ytrain = y_train, 
       xtest = x_test, 
       ytest = y_test)
})

# fit the neural network for alles sizes 
models <- list()
for(i in 1:length(dic_size)) {
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = dic_size[i]) %>%
  layer_dense(units = 16, activation = "relu") 

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
fit_history <- model %>% fit(dics[[i]]$xtrain, dics[[i]]$ytrain, epochs = 6, batch_size = 512, validation_split = 0.2)

models[[i]] <- fit_history
}

plot(models[[5]])
```


