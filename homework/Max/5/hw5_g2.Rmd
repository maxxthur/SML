---
title: "Assignment 3"
author: "Group 2"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
library(mvtnorm)
library(nnet)
library(keras)
library(reticulate)
library(patchwork)
```

## Task 1

## Task 2

## Task 3

## Task 4

## Task 5

## Task 6

## Task 7

## Task 8

```{r}
# setup
a1 <- matrix(c(3, 3), ncol = 1) # already transposed
a2 <- matrix(c(3, -3), ncol = 1) # already transposed
sigmoid <- function(v) {
  1 / (1 + exp(-v))
}

# generate X_i
X1 <- rmvnorm(mean = c(0, 0), sigma = diag(2), n = 10100)
X2 <- rmvnorm(mean = c(0, 0), sigma = diag(2), n = 10100)

# as independent sampling, first 100 are training and rest is 
fX <- sigmoid(X1 %*% a1 ) + sigmoid(X2 %*% a2)

# signal-to-noise ratio is Var(fX)/Var(noise) = 4
var_noise <- var(fX)/4

# as you didnt specify mu for noise we assume 0
noise <- rnorm(n = 10100, sd = sqrt(var_noise))

# make training data 
df <- tibble(Y = fX + noise, X1 = I(X1), X2 = I(X2))
train <- df[1:100, ]

# fit the neural nets
results <- lapply(0:10, function(size) {
  
  weights <- lapply(1:10, function(x) abs(rnorm(100, mean = 0, sd = 0.01))) # generate random numbers near 0
  
  fits <- lapply(weights, function(w) nnet(formula = Y ~ ., data = train, size = size, decay = 0.0005, skip = T, weights = w))
  preds <- lapply(fits, predict, newdata = df[-(1:100), ])
  avg_train_err <- lapply(preds, function(p) mean((df[-(1:100),]$Y - p)^2))
  data.frame(size = size, err = unlist(avg_train_err))
})

# bind the results 
results_bind <- bind_rows(results) 
means <- results_bind %>% 
  group_by(size) %>% 
  summarise(err = median(err))

# visualize with box and line plot
results_bind %>% 
  ggplot() +
  geom_boxplot(aes(x = size, y = err, group = size), width = 0.25) +
  geom_line(data = means, aes(x = size, y = err)) +
  scale_x_continuous(breaks = 0:10)

```


## Task 9

## Task 10

We first fit the fully connected neural network with two hidden layers for all dictionary sizes as the case of size 1000 is nested there. We also collect data about the evolution of the network to visualize results later. 

```{r, results='hide'}
library(keras)

# load the data for different dictionary sizes and prepare it for analysis
dic_size <- c(500, 1000, 3000, 5000, 10000)

# implwmwnr one hot encoding
vectorize_sequences <- function(sequences, dimension) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1 
  results
}

dics <- lapply(dic_size, function(d) {
  imdb <- dataset_imdb(num_words = d)
  x_train <- vectorize_sequences(imdb$train$x, dimension = d)
  x_test <- vectorize_sequences(imdb$test$x, dimension = d)
  # Also change labels from integer to numeric
  y_train <- as.numeric(imdb$train$y)
  y_test <- as.numeric(imdb$test$y)
  list(xtrain = x_train, 
       ytrain = y_train, 
       xtest = x_test, 
       ytest = y_test)
})

# fit the neural network for all sizes 
models <- list()
for(i in 1:length(dic_size)) {
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = dic_size[i]) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
fit_history <- model %>% fit(dics[[i]]$xtrain, dics[[i]]$ytrain, epochs = 20, batch_size = 512, validation_split = 0.2)

models[[i]] <- fit_history
}


# collect the histories of all models
plot_dat_list <- list()
for(i in 1:length(models)) {
  n <- models[[i]]$params$epochs
  plot_dat_list[[i]] <- tibble(loss = c(models[[i]]$metrics$loss, models[[i]]$metrics$val_loss), 
                         accuracy = c(models[[i]]$metrics$accuracy, models[[i]]$metrics$val_accuracy), 
                         name = c(rep("training", n), rep("validation", n)), 
                         epoch = rep(1:n, 2),
                         size = dic_size[i])
}

plot_dat <- bind_rows(plot_dat_list) %>% 
  pivot_longer(cols = c(loss, accuracy), 
               names_to = "metric")

```
### a)


```{r}
# plot of loss and accuracy evolving over epochs
plot_dat %>% 
  filter(size == 1000) %>% 
  ggplot(aes(x = epoch, y = value, color = name)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_wrap(~metric, nrow = 2) +
  labs(y = "") +
  theme(legend.title = element_blank())

plot_dat %>% 
  filter(size == 1000 & epoch == 20) 

```
We see that with our specification we get quite high accuracy for training and validation data sets. However, looking at the plot showing the evolution of loss and accuracy over the epochs that accuracy and loss actually diverge as the network evolves. This shows that 20 epochs might result in overfitting. 

### b)

We can now put a) in context using the results for different dictionary sizes.

We first look at the evolution of accuracy. We see that accuracy for the training and validation data set rise both up to a dictionary comprising the 3000 most frequent tokens. However, then accuracy for the validation data decreases again and stays constant thereafter just below 85 percent.

```{r}
plot_dat %>% 
  filter(epoch == 20 & metric == "accuracy") %>% 
  ggplot(aes(x = size, y = value, color = name)) +
  geom_point() +
  geom_line() +
  labs(y = "accuracy") +
  theme(legend.title = element_blank())
```

Second we want to have a look at how accuracy and loss evolve over epochs for each size. 

```{r}
# plot for loss
plot_dat %>% 
  filter(metric == "loss") %>% 
  ggplot(aes(x = epoch, y = value, color = name)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_wrap(~size, nrow = 2) +
  labs(y = "loss") +
  theme(legend.title = element_blank())

# plot for accuracy
plot_dat %>% 
  filter(metric == "accuracy") %>% 
  ggplot(aes(x = epoch, y = value, color = name)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_wrap(~size, nrow = 2) +
  labs(y = "accuracy") +
  theme(legend.title = element_blank())
```
We see that with larger dictionaries the training and test accuracies diverge more and more over the epochs. It is quite obvious that the network suffers from overfitting that becomes worse when more tokens are considered.
