---
title: "Assignment 1"
author: "Group 2"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task 1

## Task 2

## Task 3

## Task 4

## Task 5

## Task 6

First we write a function to construct $Y$ and another to calculate $EPE$ as given in the task.

```{r}
library(caret)

get_xy <- function(p, sigma, N = 500) {
  X <- sapply(1:p, function(x) runif(n = N, min = -1, max = 1))
  epsilon <- rnorm(n = N, sd = sigma)
  Y <- exp(-8 * apply(X^2, MARGIN = 1, FUN = sum)) + epsilon 
  df <- data.frame(Y = Y, X = X)
  return(df)
}

get_fx0 <- function(x0 = 0, sigma, data) {
  x0_df <- as.data.frame(matrix(rep(x0, ncol(data)-1), nrow = 1))
  names(x0_df) <- names(data)[-1]
  f <- as.formula(paste0(names(data)[1], "~", paste(names(data)[-1], collapse = "+")))
  # estimate linear model
  l <- lm(data = data, formula = f)
  l_x0 <- predict.lm(l, newdata = x0_df) # its just the intercept what was clear
                                         # but if we specify other x0 the function
                                         # still works
  # estimate knn
  knn_mod <- knnreg(formula = f, data = data, k = 1)
  knn_x0 <- predict(knn_mod, newdata = x0_df)
  
  # output the fx0 value
  data.frame(linear = l_x0, knn = as.numeric(knn_x0))
}
 
 epe <- function(fx0, x_0 = 0, p, mu = 0, sigma, f = function(x) exp(-8 * sum(x^2))) {
   x_0 <- rep(x_0, p)
   noise <- rnorm(n = nrow(fx0), mean = mu, sd = sigma)
   epe_lm <- mean((f(x_0) + noise  - fx0$linear)^2)
   epe_knn <- mean((f(x_0) + noise - fx0$knn)^2)
   return(data.frame(epe_linear = epe_lm, epe_knn = epe_knn))
}
```

Now that we have the function we can iterate over $p$ and $\sigma$ and estimate in each of the 1000 iterations a linear model as well as a a $KNN(1)$. 

```{r}
# get all combinations of p and sigma
grid <- expand.grid(p = 1:10, sigma = 0:1)

grid_split <- with(grid, split(x = grid, f = list(p, sigma)))

# create 1000 datasets per p-sigma combination

results <- lapply(grid_split, function(g) {
  # generate specific data sets
  spec <- vector(mode = "list", length = 1000)
  for(m in 1:1000) {
    spec[[m]] <- get_xy(p = g$p, sigma = g$sigma)
  }
  
  # evaluate f hat at x_0 = 0
  fx0 <- do.call("rbind", lapply(spec, function(s) get_fx0(data = s, sigma = g$sigma)))
  
  epe_run <- epe(fx0 = fx0, x_0 = 0, p = g$p, mu = 0, sigma = g$sigma)
  
  data.frame(p = g$p, sigma = g$sigma, epe_run)
})

results_bind <- do.call("rbind", results)

saveRDS(results_bind, file = "results_task6.rds")
```

We can now inspect the results of the simulation. 

```{r}
# make a nice plot
library(ggplot2)
library(tidyr)
library(ggthemes)
library(latex2exp)

results_bind |>
  pivot_longer(cols = contains("epe")) |>
  ggplot(aes(x = p, y = value, color = as.factor(sigma), group = as.factor(sigma))) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  facet_wrap(~ name, nrow = 2) +
  labs(y = "EPE", color = TeX("$\\sigma$")) +
  theme_base()
  

```
There are two observations here. First, dimensionality increases EPE for both methods. However the linear model experiences the increase earlier but then further increases become smaller while for KNN the increases are smaller but last longer with increasing $p$. Second, an increase in $\sigma$ shifts up EPE almost by its magnitude. This illustrates the irreducible error that is introduced by the noise term which is usually part of the models we think of when describing data generating processes. 

## Task 7

```{r}
# load data
data("diabetes", package = "lars")

# matrices can apparently be columns in df..
# good to know
y <- rnorm(100)
x1 = rnorm(100)
x2 = rnorm(100)
x <- cbind(x1, x2) 
df <- data.frame(y = y, x = I(x))

# but this complicates things so we break up the 
# structure and make a nice df 
# function to get rid of Asis
unAsIs <- function(X) {
    if("AsIs" %in% class(X)) {
        class(X) <- class(X)[-match("AsIs", class(X))]
    }
    X
}

# extract y and x
y <- diabetes$y
x <- unAsIs(diabetes$x)

# make a new df with all the data
diabetes_df <- as.data.frame(cbind(y, x))

```
As instructed, we now set a seed and sample row indices from the set of integers running from 1 to the number of observations with equal probability. 

```{r}
# set seed
set.seed(12)

# split the data into train and test
# step 1: sample 400 indices
ind <- sample(x = 1:nrow(diabetes_df), size = 400)

# subset the datasets as instructed
train <- diabetes_df[ind, ]
test <- diabetes_df[-ind, ]
```

The reason why random sampling is a good idea is that we are not really familiar with the dataset. Specifically we do not know whether observations were sorted by any of the variables available and if so we do not know at all by which one. Just taking the 400 first observations then would lead the information contained in training and test data to be biased by sorting leading ultimately to sampling bias in our estimations. 

INSERT EXPLANATION ABOUT STANDARDIZED VARIABLES HERE.

To analyse the correlation structures we simply calculate a matrix with correlation of all columns in our dataset. The first column contains the correlations of the variables in $X$ with $y$ and the other columns and rows respectively contain the correlations between the columns in $X$. In general high absolute values of $corr(X_k, y)$ are desirable because this implies high co- or countermovement of the dependent and independent variables. This at least hints at predictive power of $X_k$, where $k$ is the column index. Contrary, low values for $corr(X_k, X_j)$, $k \neq j$ are desirable as high values would introduce all the problems associated with multicollinearity, most prominently however the variance of the estimates will become inflated. This means nothing else than a loss in precision of estimates. Another huge problem is that multicollinearity is associated with "almost rank deficient" $X'X$ what can lead to problems if we run our model on a computer system. 

```{r}
library(kableExtra)
# exploration of correlation
correlation_matrix <- round(cor(train), 2)
# eliminate redundancies and make a nice table for the pdf
correlation_matrix[!lower.tri(correlation_matrix)] <- ""
kable(correlation_matrix, booktabs = T)
```


```{r}
# use training data to fit the full model
# get model formula from column names
f <- as.formula(paste0("y~", paste(colnames(train)[-1], collapse = "+")))

fit_full <- lm(data = train, formula = f)

# get variables significant at alpha = 0.05
summary_full <- summary(fit_full)
coefficients <- summary_full$coefficients
significant <- which(coefficients[, 4] < 0.05)[-1]

# in sample MSE
MSE_full_in <- mean(fit_full$residuals^2)

# out of sample MSE
pred_full <- predict(fit_full, newdata = test)
MSE_full_out <- mean((test$y - pred_full)^2)

message(paste("In sample MSE is", MSE_full_in, sep = ": "))
message(paste("Out of sample MSE is", MSE_full_out, sep = ": "))
```

```{r}
# use the significant variables only 
f2 <- as.formula(paste0("y~", paste(colnames(train)[significant], collapse = "+")))

# estimate smaller model
fit_sig <- lm(data = train, formula = f2)

# in sample MSE
MSE_sig_in <- mean(fit_sig$residuals^2)

# out of sample MSE
pred_sig <- predict(fit_sig, newdata = test)
MSE_sig_out <- mean((test$y - pred_sig)^2)

message(paste("In sample MSE is", MSE_sig_in, sep = ": "))
message(paste("Out of sample MSE is", MSE_sig_out, sep = ": "))

# F-Test
anova(fit_full, fit_sig)
```


## Task 8

The Akaike criterion is in general given by 

$$
  AIC = 2K - ln(L)
$$
where $K$ is the number of regressors and $L$ is the likelihood of the model. Since $2K$ is a penalty term, AIC is to be minimized to find the most appropriate model. 

### Best Subset Selection

Best subset selection is basically just getting all $2^P$ possible combinations of explanatory variables available to us and estimating the corresponding regression models. It would be conventient to use the leaps package for this task but there the AIC-criterion is not implemented but only SIC and BIC. So we have to construct the models ourselves, estimate them and calculate AIC.

First we write a function that calculates all possible combinations of regressors and gives back the associated regression formulas. We will not use the leaps and bounds algorithm as the dataset is not to big. 

```{r}
# write function to get formulas
bs_formulas <- function(x = train, dep = "y", intercept_only = T) {
  # extract variables names
  vars <- names(x)
  exps <- vars[vars != dep]
  
  # get all combinations
 f <- lapply(1:length(exps), function(k) {
    # get combinations for given k
    combinations <- combn(exps, m = k, simplify = F)
    # make it a regression formula
    formulas <- lapply(combinations, function(c) {
      paste(dep, paste(c, collapse = "+"), sep = "~")
    })
    # make it a vector again
    unlist(formulas)
    }) 
  # dissolve list again
  output <- unlist(f)
  
  if(intercept_only == T) output <- c(as.formula(paste0(dep, "~ 1")), output)
  
  return(output)
}
```

Then we estimate the models

```{r}
# get formulas
formulas <- bs_formulas() # look at defaults set above

# estimate all models
fits <- lapply(formulas, function(f) lm(data = train, formula = f))

# get log likelihoods
LL <- unlist(lapply(fits, function(f) logLik(f)[1]))

# get number of regressors (K)
K <- unlist(lapply(fits, function(fit) length(fit$coefficients)))

# get AIC
AIC <- 2 * K - 2 * LL

# get the most appropriate model
best_index <- which.min(AIC)

# display it
formulas[[best_index]]
```
Now that we have identified the best model we can assess its in- and out-of-sample performance using MSE again.

```{r}
# get in sample MSE
MSE_BS_in <- mean(fits[[best_index]]$residuals^2)

# get out of sample MSE
MSE_BS_out <- mean((test$y - predict(fits[[best_index]], newdata = test))^2)
```
Finally we conduct an F-Test of the identified model against the full model.

```{r}
anova(fits[[best_index]], fits[[length(fits)]]) # last model is the full one by construction
```

### Backward Stepwise

Here we are lucky because the MASS package provides us with a function that does stepwise regression based on AIC. 

```{r}
library(MASS)

# conduct backwards stepwise regression
backwards_step <- stepAIC(fit_full, direction = "backward")

length(backwards_step$coefficients)
length(fit_full$coefficients)

# get in sample MSE
MSE_backwards_in <- mean(backwards_step$residuals^2)

# get out of sample MSE

MSE_backwards_out <- mean((test$y - predict(backwards_step, newdata = test))^2)

anova(fit_full, backwards_step)
```


## Task 9

First we load the data. 

```{r}
data("Wage", package = "ISLR")
```

It is already in a nice data.frame such that we can directly dive into modeling. We first remove logwage and add the square of age to the data. Then we search for problematic columns by looking for constant variables, i.e. variables which have always the same value. If we find such a variable it is also deleted from the data.frame as it will cause problems when using lm().

```{r}
# exclude logwage 
Wage$logwage <- NULL

# add squared age to the data set 
Wage$age_sq <- Wage$age^2

# count distinct values for each variable 
count_uniq <- lapply(Wage, function(var) length(unique(var)))

# kick if there is a constant and print a message to know which one were  kicked
for(i in 1:length(count_uniq)) {
  if(count_uniq[[i]] == 1) {
    Wage[, names(count_uniq)[i]] <- NULL
    message(names(count_uniq)[i], " was kicked because it is a constant.")
  }
}
```

Then we estimate the full model with the Wage data.frame. We use Helmert-contrasts as we then can set $< HS \ Grad$ as the baseline and can interpret coefficients then successively in the order $< HS \ Grad < HS \ Grad < Some \ College < College \ Grad < Advanced \ Degree$ in the sense what the difference between the outcome of interest and the one below in the ranking is.

```{r, results="asis"}
# set dependent
dep <- "wage"

# set independents
indep <- names(Wage)[names(Wage) != dep]

# get model formula as string
f_full <- paste0(dep, "~", paste(indep, collapse = "+"))

# fit model with all desired explanatories
fit_full <- lm(data = Wage, formula = f_full, contrasts = list(education = "contr.helmert"))

# use stargazer for a nice regression table
library(stargazer)
stargazer(fit_full, type = "latex")
```

Now that we have the full model we can again use the function for best subset selection written for Task 8. We then use the AIC again to pick the best model 

```{r}
# get all formulas 
wage_formulas <- bs_formulas(x = Wage, dep = "wage")

# estimate all models
fits_wage <- lapply(wage_formulas, function(f) lm(data = Wage, formula = f))

# get log likelihoods
LL_wage <- unlist(lapply(fits_wage, function(f) logLik(f)[1]))

# get number of regressors (K)
K_wage <- unlist(lapply(fits_wage, function(fit) length(fit$coefficients)))

# get AIC
AIC_wage <- 2 * K_wage - 2 * LL_wage

# get the most appropriate model
best_index_wage <- which.min(AIC_wage)

# display it
wage_formulas[[best_index_wage]]

# display summary of the chosen model
summary(lm(data = Wage, formula = wage_formulas[[best_index_wage]], contrasts = list(education = "contr.helmert")))
```

ADD INTERPRETATION HERE. 

Finally, we check whether simply squaring the age variable to exploit diminishing returns of work experience yields different results from using orthogonal polynoms. We just amend the formula for the full model to achieve this. 

```{r}
f_full_orth <- sub(x = f_full, pattern = "age_sq", replacement = "poly(age, 2)")
f_full_orth <- sub(x = f_full_orth, pattern = "age\\+", replacement = "")

summary(lm(data = Wage, formula = f_full_orth))
```

We see that using orthogonal polynoms changes the coefficients of both age terms. This is due to the fact that $age^2$ is correlated with $age$. Orthogonal polynomials have on the other side have zero correlation due to the very fact that they are orthogonal to each other. This avoids multicollinearity between the two terms completely and therefore leads to more precise estimates.


## Task 10

```{r}
# implementation of forwardstagewise
forwardstagewise <- function(x, y, tol = sqrt(.Machine$double.eps)) {
  
  # checks 
  check_dim <- length(y) == nrow(x)
  check_num <- is.numeric(y) & is.numeric(x)
  
  if(check_dim == F) stop("Dimensions of x and y are not compatible.")
  if(check_dim == F) stop("x or y is not a numeric vector (y) or numeric matrix (x).")
   
  # center x and y
  y <- y - mean(y)
  x <- sweep(x, 2, colMeans(x), '-')
  
  # make y a n x 1 matrix
  y_m <- as.matrix(y)
  
  # init a coefficient vector b = 0
  b <- matrix(rep(0, ncol(x)), byrow = T)
  
  # set the residuals to y initially
  resid <- y_m
  
  # get absolute correlations between y and x as starting point
  res_cor <- abs(cor(resid, x))
  
  # initialize empty output matrix
  output <- matrix(b, ncol = nrow(b))

  count_iter <- 1
  repeat {
    # print(count_iter)
    # identify col index of variable with highest correlation
    index_high <- which.max(res_cor)
    
    # regress residuals on chosen variable
    b_add <- sum(x[, index_high] * resid) / sum(x[, index_high]^2)
    
    # add coefficient to b at the appropriate index
    b[index_high, ] <- b[index_high,] + b_add
    
    output <- rbind(output, matrix(b, ncol = nrow(b))) 
    
    # update residuals
    resid <- y_m - x %*% b 
    
    # update absolute residual correlations
    res_cor <- abs(cor(resid, x))
    
    # now check whether we stop
    if(max(res_cor) < tol) break
    count_iter <- count_iter + 1
  }
  
  # return output
  return(output)
}
```


```{r}
library(MASS)
# Simulation: NEEDS TO BE DONE 50 times!

# specify params of multivariate normal
mu <- rep(0, 31) # as standard normal has mu = 0
sigma <- diag(x = 1, nrow = 31) # sigma is 1 in standard normal
sigma[!diag(x = T, nrow = 31)] <- 0.85 # add covariances to off diags

# draw 300 obs from multivariate normal
{
  set.seed(123) # ensure that we always get the same numbers
x <- mvrnorm(n = 300, mu = mu, Sigma = sigma)
colnames(x) <- 1:31 # name by col index

# sample column indices to select randomly which of the 31 variables 
# define y
col_ind <- sort(sample(1:31, size = 10))

# draw coefficient from N(0, 0.4)
random_coef <- rnorm(n = 10, mean = 0, sd = sqrt(0.4))# assuming that you give the variance in the instructions

# get noise
noise <- rnorm(300, mean = 0, sd = sqrt(6.25))
}

# create empty matrix 
coef_mat <- matrix(rep(NA, 10 * 300), ncol = 10)

# fill columns with randomly drawn coefficients
for(i in 1:length(random_coef)) coef_mat[, i] <- random_coef[i]

# chose columns of x by drawn indices
x_chosen <- x[, col_ind]

# get row sum of hadamard product of coefficient matrix and 
# randomly selected columns from the mult. normal matrix
y <- apply(coef_mat * x_chosen, MARGIN = 1, FUN = sum)

# add noise as instructed
y_noise <- y + noise

# test forwardstagewise
test_fsw <- forwardstagewise(x = x, y = y_noise)
test_fsw[ncol(test_fsw), ]
```

```{r}
library(leaps)

# best subset selection with leaps

# center variables
y_c <- y_noise - mean(y_noise)
x_c <- sweep(x, 2, colMeans(x), '-')

# perform best subset selection (maybe force out the intercept as it should be 0!)
best_subset <- regsubsets(x = x_c, y = y_c, method = "exhaustive", nvmax = 31, id = 10)

# as we know the true process has 10 relevant variables we extract
# the model that was best for 10 variables

# get summary
summary_bs <- summary(best_subset)

# extract the which matrix
models_each_n <- summary_bs$which
best_10 <- models_each_n[10, ] # maybe we have to choose here by other criterion?

# get included variables indices
included_bs <- as.numeric(names(best_10[best_10 == T])[-1])

# compare the result with the sampled col indices
included_bs == col_ind # they are the same

# we can also 

```

```{r}

```

