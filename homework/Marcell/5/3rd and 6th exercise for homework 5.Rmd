---
title: "3rd and 6th exercises for homework5"
author: "Marcell Frisch"
date: '2023 11 02 '
output: html_document
---

# Task 3

## Determine the Bayes error for this classification problem.

#### Step 1: Logit Transformation
Starting with the logit transformation of the probability:
\[ \text{logit}(\pi(X)) = X \]

#### Step 2: Exponentiation
Exponentiating both sides to remove the logarithm:
\[ e^X = \frac{\pi(X)}{1 - \pi(X)} \]

#### Step 3: Solve for \(\pi(X)\)
Rearranging the terms to solve for \(\pi(X)\):
\[ e^X(1 - \pi(X)) = \pi(X) \]

#### Step 4: Simplification
Further simplification:
\[ e^X - e^X \pi(X) = \pi(X) \]

#### Step 5: Solve for \(\pi(X)\)
\[ \pi(X) + e^X \pi(X) = e^X \]
\[ \pi(X)(1 + e^X) = e^X \]
\[ \pi(X) = \frac{e^X}{1 + e^X} \]

#### Step 6: Probability of \(Y=1\) and \(Y=0\)
Therefore, the probability of \(Y=1\) given \(X\) is:
\[ \text{Pr}(Y=1 \mid X) = \pi(X) = \frac{e^X}{1 + e^X} \]

And the probability of \(Y=0\) given \(X\) is:
\[ \text{Pr}(Y=0 \mid X) = 1 - \pi(X) = \frac{1}{1 + e^X} \]

###  Bayes Error Definition
The Bayes error is the lowest possible error any classifier can achieve. It is the expectation of the minimum posterior probability, averaged over the distribution of \(X\):
\[ \text{Bayes Error} = \int_{-\infty}^{\infty} \min\left(\frac{e^X}{1 + e^X}, \frac{1}{1 + e^X}\right) \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx \]


```{r}
rm(list = ls())


integrand <- function(x) {
p1 <- exp(x) / (1 + exp(x))
min_p <- min(p1, 1 - p1)
min_p * (1 / sqrt(2 * pi)) * exp(-xˆ2 / 2)
}

# Perform the numerical integration
result <- integrate(integrand, lower = -Inf, upper = Inf)

# Calculate the Bayes error
bayes_error <- result$value

# Print the Bayes err

print(bayes_error)




```


## Determine the test error for the fitted model which predicts always 1.

```{r}

rm(list = ls())

# Define the corrected integrand
integrand <- function(x) {
1 / (1 + exp(x)) * (1 / sqrt(2 * pi)) * exp(-xˆ2 / 2)
}

# Perform the numerical integration over a large but finite interval
result <- integrate(integrand, lower = -Inf, upper = Inf)

# Calculate the test error
test_error <- result$value 

# Print the test error
print(test_error)



```

## Determine the test error for the fitted model which predicts 1 for positive X and 0 otherwise.


```{r}
# Define the integrands

rm(list = ls())

# Define the integrands
integrand1 <- function(x) {
  1 / (1 + exp(x)) * (1 / sqrt(2 * pi)) * exp(-x^2 / 2)
}

integrand2 <- function(x) {
  exp(x) / (1 + exp(x)) * (1 / sqrt(2 * pi)) * exp(-x^2 / 2)
}

# Perform numerical integration for each part
result1 <- integrate(integrand1, lower = 0, upper = Inf)
result2 <- integrate(integrand2, lower = -Inf, upper = 0)

# Calculate the test error
test_error <- result1$value + result2$value

# Print the test error
print(test_error)

# Reproduce the logic with a larger sample

# Set the seed for reproducibility
set.seed(123)

# Number of simulations
n <- 5000000

# Generate random numbers from a normal distribution for X
X <- rnorm(n, mean = 0, sd = 1)

# Calculate pi(X)
pi <- exp(X) / (1 + exp(X))

# Generate true labels based on pi(X)
true_labels <- rbinom(n, 1, pi)

# Calculate Bayes Error
bayes_error <- mean(pmin(pi, 1 - pi))

# Test Error for model predicting always 1
model_always_1_error <- mean(true_labels == 0)

# Test Error for model predicting 1 for positive X and 0 otherwise
predicted_labels <- as.numeric(X > 0)
model_X_based_error <- mean(predicted_labels != true_labels)

# Print errors
print(paste("Bayes Error:", bayes_error))

print(paste("Test Error when the model always predicts 1:", model_always_1_error))


print(paste("Test Error when the model predicts based on X:", model_X_based_error))
```




## Task 6


```{r}
library(randomForest)

# Function to generate binary dependent variable
generate_binary_variable <- function(X, q, J) {
  sum_X_j <- rowSums(X[, 1:J])
  probabilities <- q + (1 - 2 * q) * as.numeric(sum_X_j > J / 2)
  return(factor(rbinom(n = length(probabilities), size = 1, prob = probabilities), levels = c(0, 1)))
}

# Parameters
set.seed(123)  # Set seed for reproducibility
N_train <- 300
N_test <- 500
repetitions <- 50
q <- 0.1
J_values <- c(5, 25, 50, 100, 150)

# Results storage
misclassification_rates <- matrix(NA, nrow = repetitions, ncol = length(J_values))

# Loop over different numbers of noise predictor variables
for (j in seq_along(J_values)) {
  J <- J_values[j]
  
  for (rep in 1:repetitions) {
    # Generate training data
    X_train <- matrix(runif(N_train * (J + J), 0, 1), ncol = (J + J))
    Y_train <- generate_binary_variable(X_train, q, J)
    
    # Generate test data
    X_test <- matrix(runif(N_test * (J + J), 0, 1), ncol = (J + J))
    Y_test <- generate_binary_variable(X_test, q, J)
    
    # Fit random forest
    rf_model <- randomForest(X_train, Y_train, mtry = sqrt(ncol(X_train)))
    
    # Predict on test set
    Y_pred <- predict(rf_model, X_test)
    
    # Calculate misclassification rate
    misclassification_rate <- mean(Y_pred != Y_test)
    
    # Store the result
    misclassification_rates[rep, j] <- misclassification_rate
  }
}



```


```{r}
library(ggplot2)

# Convert misclassification_rates matrix to a data frame
misclassification_df <- data.frame(
  Repetition = rep(1:repetitions, each = length(J_values)),
  J = rep(J_values, times = repetitions),
  Misclassification_Rate = as.vector(misclassification_rates)
)

# Create a boxplot
ggplot(misclassification_df, aes(x = as.factor(J), y = Misclassification_Rate, fill = as.factor(J))) +
  geom_boxplot() +
  labs(title = "Test Misclassification Rates for Different Numbers of Noise Predictor Variables",
       x = "Number of Noise Predictor Variables (J)",
       y = "Misclassification Rate") +
  theme_minimal()
```

