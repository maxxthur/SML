---
title: "3rd exercise for homework5"
author: "Marcell Frisch"
date: '2023 11 02 '
output: html_document
---

# Task 3


## Determine the Bayes error for this classification problem.

```{r}
rm(list = ls())

# Define the logistic function
logistic_function <- function(x) exp(x) / (1 + exp(x))

# Define the integrand for X < 0
integrand_for_negative_X <- function(x) logistic_function(x) * dnorm(x)

# Define the integrand for X > 0
integrand_for_positive_X <- function(x) (1 - logistic_function(x)) * dnorm(x)

# Perform numerical integration for X < 0
bayes_error_negative <- integrate(integrand_for_negative_X, lower = -Inf, upper = 0)

# Perform numerical integration for X > 0
bayes_error_positive <- integrate(integrand_for_positive_X, lower = 0, upper = 550)

# Calculate the total Bayes Error
total_bayes_error <- bayes_error_negative$value + bayes_error_positive$value

# Print the Bayes Error
print(total_bayes_error)



```


## Determine the test error for the fitted model which predicts always 1.

```{r}

rm(list = ls())

# Define the corrected integrand
integrand <- function(x) {
1 / (1 + exp(x)) * (1 / sqrt(2 * pi)) * exp(-xË†2 / 2)
}

# Perform the numerical integration over a large but finite interval
result <- integrate(integrand, lower = -Inf, upper = Inf)

# Calculate the test error
test_error <- result$value 

# Print the test error
print(test_error)



```

## Determine the test error for the fitted model which predicts 1 for positive X and 0 otherwise.


```{r}
# Define the integrands

rm(list = ls())

# Define the integrands
integrand1 <- function(x) {
  1 / (1 + exp(x)) * (1 / sqrt(2 * pi)) * exp(-x^2 / 2)
}

integrand2 <- function(x) {
  exp(x) / (1 + exp(x)) * (1 / sqrt(2 * pi)) * exp(-x^2 / 2)
}

# Perform numerical integration for each part
result1 <- integrate(integrand1, lower = 0, upper = Inf)
result2 <- integrate(integrand2, lower = -Inf, upper = 0)

# Calculate the test error
test_error <- result1$value + result2$value

# Print the test error
print(test_error)

# Reproduce the logic with a larger sample

# Set the seed for reproducibility
set.seed(123)

# Number of simulations
n <- 5000000

# Generate random numbers from a normal distribution for X
X <- rnorm(n, mean = 0, sd = 1)

# Calculate pi(X)
pi <- exp(X) / (1 + exp(X))

# Generate true labels based on pi(X)
true_labels <- rbinom(n, 1, pi)

# Calculate Bayes Error
bayes_error <- mean(pmin(pi, 1 - pi))

# Test Error for model predicting always 1
model_always_1_error <- mean(true_labels == 0)

# Test Error for model predicting 1 for positive X and 0 otherwise
predicted_labels <- as.numeric(X > 0)
model_X_based_error <- mean(predicted_labels != true_labels)

# Print errors
print(paste("Bayes Error:", bayes_error))

print(paste("Test Error when the model always predicts 1:", model_always_1_error))


print(paste("Test Error when the model predicts based on X:", model_X_based_error))
```




## Task 6


```{r}
library(randomForest)

# Function to generate binary dependent variable
generate_binary_variable <- function(X, q, J) {
  sum_X_j <- rowSums(X[, 1:J])
  probabilities <- q + (1 - 2 * q) * as.numeric(sum_X_j > J / 2)
  return(factor(rbinom(n = length(probabilities), size = 1, prob = probabilities), levels = c(0, 1)))
}

# Parameters
set.seed(123)  # Set seed for reproducibility
N_train <- 300
N_test <- 500
repetitions <- 50
q <- 0.1
J_values <- c(5, 25, 50, 100, 150)

# Results storage
misclassification_rates <- matrix(NA, nrow = repetitions, ncol = length(J_values))

# Loop over different numbers of noise predictor variables
for (j in seq_along(J_values)) {
  J <- J_values[j]
  
  for (rep in 1:repetitions) {
    # Generate training data
    X_train <- matrix(runif(N_train * (J + J), 0, 1), ncol = (J + J))
    Y_train <- generate_binary_variable(X_train, q, J)
    
    # Generate test data
    X_test <- matrix(runif(N_test * (J + J), 0, 1), ncol = (J + J))
    Y_test <- generate_binary_variable(X_test, q, J)
    
    # Fit random forest
    rf_model <- randomForest(X_train, Y_train, mtry = sqrt(ncol(X_train)))
    
    # Predict on test set
    Y_pred <- predict(rf_model, X_test)
    
    # Calculate misclassification rate
    misclassification_rate <- mean(Y_pred != Y_test)
    
    # Store the result
    misclassification_rates[rep, j] <- misclassification_rate
  }
}



```


```{r}
library(ggplot2)

# Convert misclassification_rates matrix to a data frame
misclassification_df <- data.frame(
  Repetition = rep(1:repetitions, each = length(J_values)),
  J = rep(J_values, times = repetitions),
  Misclassification_Rate = as.vector(misclassification_rates)
)

# Create a boxplot
ggplot(misclassification_df, aes(x = as.factor(J), y = Misclassification_Rate, fill = as.factor(J))) +
  geom_boxplot() +
  labs(title = "Test Misclassification Rates for Different Numbers of Noise Predictor Variables",
       x = "Number of Noise Predictor Variables (J)",
       y = "Misclassification Rate") +
  theme_minimal()
```

