---
title: "Assignment 1"
author: "Group 2"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task 1
Consider a regression problem with inputs \(x_i\) and outputs \(y_i\), and a parameterized model \(f_\theta(x)\) to be fit with least squares. In this problem, for each input \(x_i\), there are \(J\) repeated outputs \(y_{ij}\), where \(j = 1, \ldots, J\). To obtain the fit, we can use a least squares problem involving only \(x_i\) and the average values \(y_i = \frac{1}{J} \sum_{j=1}^{J} y_{ij}\).

\section{Derivation}

To demonstrate this, let's assume that there are observations with tied or identical values of \(x_i\). Without loss of generality, assume that \(x_1 = x_2\), and all other observations are unique. Then, the residual sum of squares (RSS) function in the general least-squares estimation is given by:

\[
\text{RSS}(\theta) = \sum_{i=1}^{N} (y_i - f_\theta(x_i))^2
\]

We can rewrite this as:

\[
\text{RSS}(\theta) = \sum_{i=1}^{N} \sum_{j=1}^{J} (y_{ij} - f_\theta(x_i))^2
\]

Since \(x_1 = x_2\), we can write:

\[
\text{RSS}(\theta) = \sum_{i=2}^{N} \sum_{j=1}^{J} (y_{ij} - f_\theta(x_i))^2 + \sum_{j=1}^{J} (y_{1j} - f_\theta(x_1))^2
\]

We can then define weights \(w_i\) as follows:

\[
w_i = 
\begin{cases}
2, & \text{if } i = 1 \text{ or } i = 2 \\
1, & \text{otherwise}
\end{cases}
\]

Using these weights, we can rewrite the RSS function as:

\[
\text{RSS}(\theta) = \sum_{i=1}^{N} w_i(y_i - f_\theta(x_i))^2
\]

This is a reduced weighted least squares estimation, which shows that the fit can be obtained from a least squares problem involving only \(x_i\) and the average values \(y_i\).


How does the least squares problem change?

Unequal Weights: In an unbalanced design, each observation (combination of $x_{i}$ and $y_{ij} $) does not carry the same weight in the least squares objective function. Inputs with more repetitions contribute more to the objective function than those with fewer repetitions. This is because there are more data points associated with inputs that have more repetitions.

Varying Variances: The assumption of equal variances across all observations may not hold in an unbalanced design. Inputs with more repetitions tend to have smaller variances because they provide more information about the underlying population. In contrast, inputs with fewer repetitions have larger variances, indicating more uncertainty in their measurements.

Covariance Structure: The covariance structure of the response variables (outputs) may become more complex in unbalanced designs. Correlation between repeated measurements within the same input can exist, and the correlations may vary across different inputs.


## Task 2

The question asks to show that the expected value of the MSE of the ordinary least squares estimate obtained for the training data is less than or equal to the expected value of the MSE of the same estimate for the test data. The MSE is the mean of the squared differences between the predicted and actual values of the response variable. The MSE is calculated using the training data for the training MSE and the test data for the test MSE.
To show that the expected value of the MSE of the ordinary least squares estimate obtained for the training data is less than or equal to the expected value of the MSE of the same estimate for the test data, we need to show that:

$$
\mathrm{E}\left[\frac{1}{N} \sum_{i=1}^N\left(y_i-\boldsymbol{x}_i^{\top} \hat{\beta}\right)^2\right] \leq \mathrm{E}\left[\frac{1}{M} \sum_{i=1}^M\left(y_i-\boldsymbol{x}_i^{\top} \hat{\beta}\right)^2\right]
$$

Taking the expectation of both sides, we get:

$$
\mathrm{E}\left[\mathrm{E}\left[\frac{1}{N} \sum_{i=1}^N\left(y_i-\boldsymbol{x}_i^{\top} \hat{\beta}\right)^2\right]\right] \leq \mathrm{E}\left[\mathrm{E}\left[\frac{1}{M} \sum_{i=1}^M\left(y_i-\boldsymbol{x}_i^{\top} \hat{\beta}\right)^2\right]\right]
$$

By the law of iterated expectations, we can simplify this to:

$$
\mathrm{E}\left[\frac{1}{N} \sum_{i=1}^N \mathrm{E}\left[\left(y_i-\boldsymbol{x}_i^{\top} \hat{\beta}\right)^2\right]\right] \leq \mathrm{E}\left[\frac{1}{M} \sum_{i=1}^M \mathrm{E}\left[\left(y_i-\boldsymbol{x}_i^{\top} \hat{\beta}\right)^2\right]\right]
$$

Since the expectations are over all that is random in each expression, we can replace them with their respective variances:

$$
\frac{1}{N} \sum_{i=1}^N \operatorname{Var}\left(y_i-\boldsymbol{x}_i^{\top} \hat{\beta}\right) \leq \frac{1}{M} \sum_{i=1}^M \operatorname{Var}\left(y_i-\boldsymbol{x}_i^{\top} \hat{\beta}\right)
$$

Expanding the variances, we get:

$$
\begin{aligned}
& \frac{1}{N} \sum_{i=1}^N \operatorname{Var}\left(y_i\right)+\operatorname{Var}\left(\boldsymbol{x}_i^{\top} \hat{\beta}\right)-2 \operatorname{Cov}\left(y_i, \boldsymbol{x}_i^{\top} \hat{\beta}\right) \leq \frac{1}{M} \sum_{i=1}^M \operatorname{Var}\left(y_i\right)+\operatorname{Var}\left(\boldsymbol{x}_i^{\top} \hat{\beta}\right)- \\
& 2 \operatorname{Cov}\left(y_i, \boldsymbol{x}_i^{\top} \hat{\beta}\right)
\end{aligned}
$$

Since the training and test data are drawn at random from the same population, we can assume that they have the same distribution and therefore the same variances and covariances. Thus, we can simplify the inequality to:

$$
\frac{1}{N} \sum_{i=1}^N \operatorname{Var}\left(y_i\right) \leq \frac{1}{M} \sum_{i=1}^M \operatorname{Var}\left(y_i\right)
$$

This inequality holds because the training data is a subset of the test data, so the variance of the training data is expected to be less than or equal to the variance of the test data. Therefore, we have shown that the expected value of the MSE of the ordinary least squares estimate obtained for the training data is less than or equal to the expected value of the MSE of the same estimate for the test data.

## Task 3
# I am absolutely not sure about the proof but if I did not screw up the equations it should be fine :D 

We have:

\[Y = \beta_0 + \beta_1X + \epsilon\]

where:
\begin{align*}
Y & \text{ is the dependent variable,} \\
X & \text{ is the independent variable,} \\
\beta_0 & \text{ and } \beta_1 \text{ are the regression coefficients, and} \\
\epsilon & \text{ is the error term.}
\end{align*}

The predicted value of $Y$ can be expressed as:

\[\hat{Y} = \beta_0 + \beta_1X\]

The sum of squared errors (SSE) can be expressed as:

\[SSE = \sum(Y_i - \hat{Y}_i)^2\]

The total sum of squares (SST) can be expressed as:

\[SST = \sum(Y_i - \bar{Y})^2\]

where:
\begin{align*}
Y_i & \text{ is the observed value of } Y, \\
\hat{Y}_i & \text{ is the predicted value of } Y, \\
\bar{Y} & \text{ is the mean of } Y.
\end{align*}

The coefficient of determination ($R^2$) is defined as:

\[R^2 = 1 - \frac{SSE}{SST}\]

To find the values of $\beta_0$ and $\beta_1$ that minimize SSE, we take the partial derivatives of SSE with respect to $\beta_0$ and $\beta_1$ and set them equal to zero. This gives us the following equations:

\[\sum Y_i = n\beta_0 + \beta_1\sum X_i\]

\[\sum(Y_iX_i) = \beta_0\sum X_i + \beta_1\sum(X_i^2)\]

Solving for $\beta_0$ and $\beta_1$, we get:

\[\beta_1 = \frac{\sum(X_i - \bar{X})(Y_i - \bar{Y})}{\sum(X_i - \bar{X})^2}\]

\[\beta_0 = \bar{Y} - \beta_1\bar{X}\]

where:
\begin{align*}
\bar{X} & \text{ is the mean of } X, \\
\bar{Y} & \text{ is the mean of } Y.
\end{align*}

The correlation coefficient between $X$ and $Y$ can be expressed as:

\[r_{xy} = \frac{\sum(X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum(X_i - \bar{X})^2 \sum(Y_i - \bar{Y})^2}}\]

Substituting the values of $\beta_0$ and $\beta_1$ into the formula for $\hat{Y}_i$, we get:

\[\hat{Y}_i = \beta_0 + \beta_1X_i = \bar{Y} + r_{xy} \left(Y_i - \bar{Y}\right) / (s_Y \sqrt{s_X^2})\]

where:
\begin{align*}
s_X & \text{ is the standard deviation of } X, \\
s_Y & \text{ is the standard deviation of } Y.
\end{align*}

Substituting this into the formula for SST, we get:

\[SST = \sum(Y_i - \bar{Y})^2 = \sum(Y_i - \hat{Y}_i + \hat{Y}_i - \bar{Y})^2 = \sum(Y_i - \hat{Y}_i)^2 + \sum(\hat{Y}_i - \bar{Y})^2\]

The second term in this equation can be expressed as:

\[\sum(\hat{Y}_i - \bar{Y})^2 = r_{xy}^2 \sum(Y_i - \bar{Y})^2 / (s_Y^2)\]

Substituting this into the formula for $R^2$, we get:

\[R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{\sum(Y_i - \hat{Y}_i)^2}{\sum(Y_i - \bar{Y})^2} = r_{xy}^2\]

Therefore, we have shown that the coefficient of determination given by the $R^2$ statistic is equal to the square of the correlation between $X$ and $Y$ in the simple linear regression case, where $Y = \beta_0 + \beta_1X + \epsilon$ and the regression coefficients $(\beta_0, \beta_1)$ are estimated using ordinary least squares.



## Task 4

## Task 5

## Task 6

## Task 7

```{r}
# load data
data("diabetes", package = "lars")

# matrices can apparently be columns in df..
# good to know
y <- rnorm(100)
x1 = rnorm(100)
x2 = rnorm(100)
x <- cbind(x1, x2) 
df <- data.frame(y = y, x = I(x))

# but this complicates things so we break up the 
# structure and make a nice df 
# function to get rid of Asis
unAsIs <- function(X) {
    if("AsIs" %in% class(X)) {
        class(X) <- class(X)[-match("AsIs", class(X))]
    }
    X
}

# extract y and x
y <- diabetes$y
x <- unAsIs(diabetes$x)

# make a new df with all the data
diabetes_df <- as.data.frame(cbind(y, x))

```
As instructed, we now set a seed and sample row indices from the set of integers running from 1 to the number of observations with equal probability. 

```{r}
# set seed
set.seed(12)

# split the data into train and test
# step 1: sample 400 indices
ind <- sample(x = 1:nrow(diabetes_df), size = 400)

# subset the datasets as instructed
train <- diabetes_df[ind, ]
test <- diabetes_df[-ind, ]
```

The reason why random sampling is a good idea is that we are not really familiar with the dataset. Specifically we do not know whether observations were sorted by any of the variables available and if so we do not know at all by which one. Just taking the 400 first observations then would lead the information contained in training and test data to be biased by sorting leading ultimately to sampling bias in our estimations. 

INSERT EXPLANATION ABOUT STANDARDIZED VARIABLES HERE.

To analyse the correlation structures we simply calculate a matrix with correlation of all columns in our dataset. The first column contains the correlations of the variables in $X$ with $y$ and the other columns and rows respectively contain the correlations between the columns in $X$. In general high absolute values of $corr(X_k, y)$ are desirable because this implies high co- or countermovement of the dependent and independent variables. This at least hints at predictive power of $X_k$, where $k$ is the column index. Contrary, low values for $corr(X_k, X_j)$, $k \neq j$ are desirable as high values would introduce all the problems associated with multicollinearity, most prominently however the variance of the estimates will become inflated. This means nothing else than a loss in precision of estimates. Another huge problem is that multicollinearity is associated with "almost rank deficient" $X'X$ what can lead to problems if we run our model on a computer system. 

```{r}
library(kableExtra)
# exploration of correlation
correlation_matrix <- round(cor(train), 2)
# eliminate redundancies and make a nice table for the pdf
correlation_matrix[!lower.tri(correlation_matrix)] <- ""
kable(correlation_matrix, booktabs = T)
```


```{r}
# use training data to fit the full model
# get model formula from column names
f <- as.formula(paste0("y~", paste(colnames(train)[-1], collapse = "+")))

fit_full <- lm(data = train, formula = f)

# get variables significant at alpha = 0.05
summary_full <- summary(fit_full)
coefficients <- summary_full$coefficients
significant <- which(coefficients[, 4] < 0.05)[-1]

# in sample MSE
MSE_full_in <- mean(fit_full$residuals^2)

# out of sample MSE
pred_full <- predict(fit_full, newdata = test)
MSE_full_out <- mean((test$y - pred_full)^2)

message(paste("In sample MSE is", MSE_full_in, sep = ": "))
message(paste("Out of sample MSE is", MSE_full_out, sep = ": "))
```

```{r}
# use the significant variables only 
f2 <- as.formula(paste0("y~", paste(colnames(train)[significant], collapse = "+")))

# estimate smaller model
fit_sig <- lm(data = train, formula = f2)

# in sample MSE
MSE_sig_in <- mean(fit_sig$residuals^2)

# out of sample MSE
pred_sig <- predict(fit_sig, newdata = test)
MSE_sig_out <- mean((test$y - pred_sig)^2)

message(paste("In sample MSE is", MSE_sig_in, sep = ": "))
message(paste("Out of sample MSE is", MSE_sig_out, sep = ": "))

# F-Test
anova(fit_full, fit_sig)
```


## Task 8

The Akaike criterion is in general given by 

$$
  AIC = 2K - ln(L)
$$
where $K$ is the number of regressors and $L$ is the likelihood of the model. Since $2K$ is a penalty term, AIC is to be minimized to find the most appropriate model. 

### Best Subset Selection

Best subset selection is basically just getting all $2^P$ possible combinations of explanatory variables available to us and estimating the corresponding regression models. It would be conventient to use the leaps package for this task but there the AIC-criterion is not implemented but only SIC and BIC. So we have to construct the models ourselves, estimate them and calculate AIC.

First we write a function that calculates all possible combinations of regressors and gives back the associated regression formulas. We will not use the leaps and bounds algorithm as the dataset is not to big. 

```{r}
# write function to get formulas
bs_formulas <- function(x = train, dep = "y", intercept_only = T) {
  # extract variables names
  vars <- names(x)
  exps <- vars[vars != dep]
  
  # get all combinations
 f <- lapply(1:length(exps), function(k) {
    # get combinations for given k
    combinations <- combn(exps, m = k, simplify = F)
    # make it a regression formula
    formulas <- lapply(combinations, function(c) {
      paste(dep, paste(c, collapse = "+"), sep = "~")
    })
    # make it a vector again
    unlist(formulas)
    }) 
  # dissolve list again
  output <- unlist(f)
  
  if(intercept_only == T) output <- c(as.formula("y ~ 1"), output)
  
  return(output)
}
```

Then we estimate the models

```{r}
# get formulas
formulas <- bs_formulas() # look at defaults set above

# estimate all models
fits <- lapply(formulas, function(f) lm(data = train, formula = f))

# get log likelihoods
LL <- unlist(lapply(fits, function(f) logLik(f)[1]))

# get number of regressors (K)
K <- unlist(lapply(fits, function(fit) length(fit$coefficients)))

# get AIC
AIC <- 2 * K - 2 * LL

# get the most appropriate model
best_index <- which.min(AIC)

# display it
formulas[[best_index]]
```
Now that we have identified the best model we can assess its in- and out-of-sample performance using MSE again.

```{r}
# get in sample MSE
MSE_BS_in <- mean(fits[[best_index]]$residuals^2)

# get out of sample MSE
MSE_BS_out <- mean((test$y - predict(fits[[best_index]], newdata = test))^2)
```
Finally we conduct an F-Test of the identified model against the full model.

```{r}
anova(fits[[best_index]], fits[[length(fits)]]) # last model is the full one by construction
```

### Backward Stepwise

Here we are lucky because the MASS package provides us with a function that does stepwise regression based on AIC. 

```{r}
library(MASS)

# conduct backwards stepwise regression
backwards_step <- stepAIC(fit_full, direction = "backward")

# get in sample MSE
MSE_backwards_in <- mean(backwards_step$residuals^2)

# get out of sample MSE

MSE_backwards_out <- mean((test$y - predict(backwards_step, newdata = test))^2)

anova(fit_full, backwards_step)
```


## Task 9

## Task 10
```{r}
forwardstagewise <- function(x, y, tol = sqrt(.Machine$double.eps)) {
 
  if (!is.matrix(x)) {
    stop("x must be a matrix")
  }
  if (!is.numeric(y)) {
    stop("y must be numeric")
  }
  if (length(y) != nrow(x)) {
    stop("The number of rows in x must be equal to the length of y")
  }
  
  
  x_centered <- scale(x, center = TRUE, scale = FALSE)
  y_centered <- scale(y, center = TRUE, scale = FALSE)
  
  
  beta <- rep(0, ncol(x))
  r <- y_centered
  
 
  coef_matrix <- matrix(0, ncol = ncol(x), nrow = ncol(x))
  
  # forward stagewise regression
  for (j in 1:ncol(x)) {

    # Compute the correlation between the residuals and each predictor
    corr <- apply(x_centered, 2, function(z) cor(z, r))
    
    
    j_star <- which.max(abs(corr))
    
    # Update the coefficients and residuals
    beta[j_star] <- beta[j_star] + corr[j_star] * tol
    r <- r - x_centered[, j_star] * corr[j_star] * tol
    
    # Store the coefficients in the matrix
    coef_matrix[j, ] <- beta
  }
  
  # Return the matrix of coefficients
  return(coef_matrix)
}
#example 
set.seed(123)
x <- matrix(rnorm(100), ncol = 5)
y <- rnorm(20)
forwardstagewise(x, y, tol = 0.01)


#this is what I have so far 
```
